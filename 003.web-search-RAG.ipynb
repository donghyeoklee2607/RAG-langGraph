{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1293b5",
   "metadata": {},
   "source": [
    "# 003. Web Search Rag with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b985c71",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab85c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is tracing....\n",
      "[Project Name]\n",
      "web-search-RAG\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from tools import logging\n",
    "import nest_asyncio\n",
    "import os, re\n",
    "\n",
    "load_dotenv(override=True)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.langsmith(\"web-search-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c502cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31194296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.21 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 1)) (0.3.21)\n",
      "Requirement already satisfied: langchain-core==0.3.46 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 2)) (0.3.46)\n",
      "Requirement already satisfied: langchain-experimental==0.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: langchain-community==0.3.20 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 4)) (0.3.20)\n",
      "Requirement already satisfied: langchain-openai==0.3.9 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 5)) (0.3.9)\n",
      "Requirement already satisfied: langchain-anthropic==0.3.10 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 6)) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters==0.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 7)) (0.3.7)\n",
      "Requirement already satisfied: langchain-elasticsearch==0.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 8)) (0.3.2)\n",
      "Requirement already satisfied: langchain-chroma==0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 9)) (0.2.2)\n",
      "Requirement already satisfied: langchain-cohere==0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: langchain-milvus==0.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 11)) (0.1.8)\n",
      "Requirement already satisfied: langchain-google-genai==2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: langchain-huggingface==0.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 13)) (0.1.2)\n",
      "Requirement already satisfied: langchain-azure-ai==0.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 14)) (0.1.2)\n",
      "Requirement already satisfied: langchain-teddynote==0.3.44 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 15)) (0.3.44)\n",
      "Requirement already satisfied: langchainhub==0.1.21 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 16)) (0.1.21)\n",
      "Requirement already satisfied: langgraph==0.3.18 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 17)) (0.3.18)\n",
      "Requirement already satisfied: langsmith==0.3.18 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 18)) (0.3.18)\n",
      "Requirement already satisfied: huggingface-hub==0.29.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 19)) (0.29.3)\n",
      "Requirement already satisfied: openai==1.67.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 20)) (1.67.0)\n",
      "Requirement already satisfied: deepl==1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 21)) (1.21.1)\n",
      "Requirement already satisfied: kiwipiepy==0.20.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 22)) (0.20.4)\n",
      "Requirement already satisfied: konlpy==0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 23)) (0.6.0)\n",
      "Requirement already satisfied: pandas==2.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 25)) (2.2.3)\n",
      "Requirement already satisfied: rank-bm25==0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 26)) (0.2.2)\n",
      "Requirement already satisfied: redis==5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 28)) (5.2.1)\n",
      "Requirement already satisfied: chromadb==0.6.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 29)) (0.6.3)\n",
      "Requirement already satisfied: pymupdf==1.25.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 31)) (1.25.4)\n",
      "Requirement already satisfied: pypdf==4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 32)) (4.3.1)\n",
      "Requirement already satisfied: pdfplumber==0.11.5 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 33)) (0.11.5)\n",
      "Requirement already satisfied: pdfminer-six==20231228 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 34)) (20231228)\n",
      "Requirement already satisfied: pymupdf4llm==0.0.17 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 35)) (0.0.17)\n",
      "Requirement already satisfied: matplotlib==3.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 37)) (3.10.1)\n",
      "Requirement already satisfied: streamlit==1.43.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 38)) (1.43.2)\n",
      "Requirement already satisfied: jupyter==1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 39)) (1.1.1)\n",
      "Requirement already satisfied: notebook==7.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 40)) (7.3.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 42)) (2.6.0)\n",
      "Requirement already satisfied: torchvision==0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 43)) (0.21.0)\n",
      "Requirement already satisfied: faiss-cpu==1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 44)) (1.10.0)\n",
      "Requirement already satisfied: open-clip-torch==2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 45)) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 47)) (1.0.1)\n",
      "Requirement already satisfied: pydantic==2.10.6 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 48)) (2.10.6)\n",
      "Requirement already satisfied: lxml==5.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 49)) (5.3.1)\n",
      "Requirement already satisfied: pillow==10.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 50)) (10.4.0)\n",
      "Requirement already satisfied: lark==1.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 51)) (1.2.2)\n",
      "Requirement already satisfied: ragas==0.2.14 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 52)) (0.2.14)\n",
      "Requirement already satisfied: unstructured==0.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 53)) (0.17.0)\n",
      "Requirement already satisfied: arxiv==2.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 54)) (2.1.3)\n",
      "Requirement already satisfied: tiktoken==0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 55)) (0.9.0)\n",
      "Requirement already satisfied: tenacity==8.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 56)) (8.5.0)\n",
      "Requirement already satisfied: pymilvus==2.5.5 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 57)) (2.5.5)\n",
      "Requirement already satisfied: google-search-results==2.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 58)) (2.4.2)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 59)) (3.20.3)\n",
      "Requirement already satisfied: sqlalchemy==2.0.39 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 60)) (2.0.39)\n",
      "Requirement already satisfied: llama-index-core==0.11.23 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 61)) (0.11.23)\n",
      "Requirement already satisfied: llama-parse==0.6.4.post1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 62)) (0.6.4.post1)\n",
      "Requirement already satisfied: llama-index-readers-file==0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 63)) (0.2.2)\n",
      "Requirement already satisfied: flashrank==0.2.10 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 64)) (0.2.10)\n",
      "Requirement already satisfied: docx2txt==0.8 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 65)) (0.8)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 66)) (1.6.0)\n",
      "Requirement already satisfied: rapidocr-onnxruntime==1.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 67)) (1.4.4)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 68)) (0.13.2)\n",
      "Requirement already satisfied: grandalf==0.8 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 69)) (0.8)\n",
      "Requirement already satisfied: rouge-score==0.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 70)) (0.1.2)\n",
      "Requirement already satisfied: langchain-ollama==0.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 71)) (0.2.3)\n",
      "Requirement already satisfied: mypy==1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 72)) (1.15.0)\n",
      "Requirement already satisfied: pinecone==5.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 73)) (5.4.2)\n",
      "Requirement already satisfied: wikipedia==1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 74)) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements_new.txt (line 75)) (1.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.3.21->-r requirements_new.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.3.21->-r requirements_new.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core==0.3.46->-r requirements_new.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core==0.3.46->-r requirements_new.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core==0.3.46->-r requirements_new.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community==0.3.20->-r requirements_new.txt (line 4)) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community==0.3.20->-r requirements_new.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community==0.3.20->-r requirements_new.txt (line 4)) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community==0.3.20->-r requirements_new.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community==0.3.20->-r requirements_new.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: anthropic<1,>=0.49.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-anthropic==0.3.10->-r requirements_new.txt (line 6)) (0.75.0)\n",
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch==0.3.2->-r requirements_new.txt (line 8)) (8.19.2)\n",
      "Requirement already satisfied: cohere<6.0,>=5.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-cohere==0.4.3->-r requirements_new.txt (line 10)) (5.20.0)\n",
      "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-cohere==0.4.3->-r requirements_new.txt (line 10)) (6.0.12.20250915)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (0.6.18)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface==0.1.2->-r requirements_new.txt (line 13)) (5.1.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface==0.1.2->-r requirements_new.txt (line 13)) (0.21.4)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface==0.1.2->-r requirements_new.txt (line 13)) (4.50.3)\n",
      "Requirement already satisfied: azure-ai-inference<2.0.0,>=1.0.0b7 in /opt/anaconda3/lib/python3.12/site-packages (from azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (1.0.0b9)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (1.36.0)\n",
      "Requirement already satisfied: azure-cosmos<5.0.0,>=4.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (4.14.2)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (1.25.1)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (4.15.5)\n",
      "Requirement already satisfied: simsimd<7.0.0,>=6.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (6.5.3)\n",
      "Requirement already satisfied: pinecone-client[grpc] in /opt/anaconda3/lib/python3.12/site-packages (from langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (3.2.2)\n",
      "Requirement already satisfied: pinecone-text in /opt/anaconda3/lib/python3.12/site-packages (from langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (0.11.0)\n",
      "Requirement already satisfied: olefile in /opt/anaconda3/lib/python3.12/site-packages (from langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (0.47)\n",
      "Requirement already satisfied: pdf2image in /opt/anaconda3/lib/python3.12/site-packages (from langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (1.17.0)\n",
      "Requirement already satisfied: feedparser in /opt/anaconda3/lib/python3.12/site-packages (from langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (6.0.12)\n",
      "Requirement already satisfied: tavily-python in /opt/anaconda3/lib/python3.12/site-packages (from langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (0.7.13)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchainhub==0.1.21->-r requirements_new.txt (line 16)) (2.32.4.20250913)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph==0.3.18->-r requirements_new.txt (line 17)) (2.1.2)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph==0.3.18->-r requirements_new.txt (line 17)) (0.1.8)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph==0.3.18->-r requirements_new.txt (line 17)) (0.1.74)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith==0.3.18->-r requirements_new.txt (line 18)) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith==0.3.18->-r requirements_new.txt (line 18)) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith==0.3.18->-r requirements_new.txt (line 18)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith==0.3.18->-r requirements_new.txt (line 18)) (0.23.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub==0.29.3->-r requirements_new.txt (line 19)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub==0.29.3->-r requirements_new.txt (line 19)) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub==0.29.3->-r requirements_new.txt (line 19)) (4.66.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai==1.67.0->-r requirements_new.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai==1.67.0->-r requirements_new.txt (line 20)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai==1.67.0->-r requirements_new.txt (line 20)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai==1.67.0->-r requirements_new.txt (line 20)) (1.3.0)\n",
      "Requirement already satisfied: kiwipiepy_model<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from kiwipiepy==0.20.4->-r requirements_new.txt (line 22)) (0.20.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from konlpy==0.6.0->-r requirements_new.txt (line 23)) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements_new.txt (line 25)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements_new.txt (line 25)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements_new.txt (line 25)) (2023.3)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.3.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.124.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.38.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (7.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (4.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb==0.6.3->-r requirements_new.txt (line 29)) (13.7.1)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfplumber==0.11.5->-r requirements_new.txt (line 33)) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer-six==20231228->-r requirements_new.txt (line 34)) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer-six==20231228->-r requirements_new.txt (line 34)) (43.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.1->-r requirements_new.txt (line 37)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.1->-r requirements_new.txt (line 37)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.1->-r requirements_new.txt (line 37)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.1->-r requirements_new.txt (line 37)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.1->-r requirements_new.txt (line 37)) (3.1.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (8.1.7)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (22.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit==1.43.2->-r requirements_new.txt (line 38)) (6.4.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/anaconda3/lib/python3.12/site-packages (from jupyter==1.1.1->-r requirements_new.txt (line 39)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/anaconda3/lib/python3.12/site-packages (from jupyter==1.1.1->-r requirements_new.txt (line 39)) (7.16.4)\n",
      "Requirement already satisfied: ipykernel in /opt/anaconda3/lib/python3.12/site-packages (from jupyter==1.1.1->-r requirements_new.txt (line 39)) (6.28.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (from jupyter==1.1.1->-r requirements_new.txt (line 39)) (7.8.1)\n",
      "Requirement already satisfied: jupyterlab in /opt/anaconda3/lib/python3.12/site-packages (from jupyter==1.1.1->-r requirements_new.txt (line 39)) (4.3.8)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from notebook==7.3.3->-r requirements_new.txt (line 40)) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from notebook==7.3.3->-r requirements_new.txt (line 40)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from notebook==7.3.3->-r requirements_new.txt (line 40)) (0.2.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->-r requirements_new.txt (line 42)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->-r requirements_new.txt (line 42)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->-r requirements_new.txt (line 42)) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->-r requirements_new.txt (line 42)) (1.13.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from open-clip-torch==2.31.0->-r requirements_new.txt (line 45)) (2024.9.11)\n",
      "Requirement already satisfied: ftfy in /opt/anaconda3/lib/python3.12/site-packages (from open-clip-torch==2.31.0->-r requirements_new.txt (line 45)) (6.3.1)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from open-clip-torch==2.31.0->-r requirements_new.txt (line 45)) (0.7.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (from open-clip-torch==2.31.0->-r requirements_new.txt (line 45)) (1.0.22)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic==2.10.6->-r requirements_new.txt (line 48)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic==2.10.6->-r requirements_new.txt (line 48)) (2.27.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (from ragas==0.2.14->-r requirements_new.txt (line 52)) (4.4.1)\n",
      "Requirement already satisfied: appdirs in /opt/anaconda3/lib/python3.12/site-packages (from ragas==0.2.14->-r requirements_new.txt (line 52)) (1.4.4)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/anaconda3/lib/python3.12/site-packages (from ragas==0.2.14->-r requirements_new.txt (line 52)) (5.6.3)\n",
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (4.0.0)\n",
      "Requirement already satisfied: python-magic in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (0.4.27)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (4.12.3)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (2.15.0)\n",
      "Requirement already satisfied: python-iso639 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (2025.11.16)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (3.14.3)\n",
      "Requirement already satisfied: backoff in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (0.32.3)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (1.14.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /opt/anaconda3/lib/python3.12/site-packages (from unstructured==0.17.0->-r requirements_new.txt (line 53)) (1.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus==2.5.5->-r requirements_new.txt (line 57)) (5.10.0)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus==2.5.5->-r requirements_new.txt (line 57)) (2.5.1)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core==0.11.23->-r requirements_new.txt (line 61)) (1.3.1)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core==0.11.23->-r requirements_new.txt (line 61)) (1.0.8)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core==0.11.23->-r requirements_new.txt (line 61)) (0.9.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.4 in /opt/anaconda3/lib/python3.12/site-packages (from llama-parse==0.6.4.post1->-r requirements_new.txt (line 62)) (0.6.21)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file==0.2.2->-r requirements_new.txt (line 63)) (0.0.26)\n",
      "Requirement already satisfied: pyclipper>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr-onnxruntime==1.4.4->-r requirements_new.txt (line 67)) (1.4.0)\n",
      "Requirement already satisfied: opencv-python>=4.5.1.48 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr-onnxruntime==1.4.4->-r requirements_new.txt (line 67)) (4.11.0.86)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr-onnxruntime==1.4.4->-r requirements_new.txt (line 67)) (1.16.0)\n",
      "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr-onnxruntime==1.4.4->-r requirements_new.txt (line 67)) (2.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from rouge-score==0.1.2->-r requirements_new.txt (line 70)) (2.2.2)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-ollama==0.2.3->-r requirements_new.txt (line 71)) (0.6.1)\n",
      "Requirement already satisfied: mypy_extensions>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from mypy==1.15.0->-r requirements_new.txt (line 72)) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/anaconda3/lib/python3.12/site-packages (from pinecone==5.4.2->-r requirements_new.txt (line 73)) (2025.4.26)\n",
      "Requirement already satisfied: pinecone-plugin-inference<4.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pinecone==5.4.2->-r requirements_new.txt (line 73)) (3.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from pinecone==5.4.2->-r requirements_new.txt (line 73)) (0.0.7)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /opt/anaconda3/lib/python3.12/site-packages (from pinecone==5.4.2->-r requirements_new.txt (line 73)) (2.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn==1.6.1->-r requirements_new.txt (line 75)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn==1.6.1->-r requirements_new.txt (line 75)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn==1.6.1->-r requirements_new.txt (line 75)) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->-r requirements_new.txt (line 42)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit==1.43.2->-r requirements_new.txt (line 38)) (4.23.0)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit==1.43.2->-r requirements_new.txt (line 38)) (0.12.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /opt/anaconda3/lib/python3.12/site-packages (from anthropic<1,>=0.49.0->langchain-anthropic==0.3.10->-r requirements_new.txt (line 6)) (0.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==1.67.0->-r requirements_new.txt (line 20)) (3.7)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from azure-ai-inference<2.0.0,>=1.0.0b7->azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (0.7.2)\n",
      "Requirement already satisfied: azure-core-tracing-opentelemetry in /opt/anaconda3/lib/python3.12/site-packages (from azure-ai-inference[opentelemetry]<2.0.0,>=1.0.0b7->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (1.0.0b12)\n",
      "Requirement already satisfied: msal>=1.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity<2.0.0,>=1.15.0->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from azure-identity<2.0.0,>=1.15.0->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->unstructured==0.17.0->-r requirements_new.txt (line 53)) (2.5)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.2.0)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /opt/anaconda3/lib/python3.12/site-packages (from cohere<6.0,>=5.12.0->langchain-cohere==0.4.3->-r requirements_new.txt (line 10)) (1.12.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer-six==20231228->-r requirements_new.txt (line 34)) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (3.26.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch==0.3.2->-r requirements_new.txt (line 8)) (8.17.1)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.0.4)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.12/site-packages (from feedparser->langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (1.0.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.43.2->-r requirements_new.txt (line 38)) (4.0.7)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (1.34.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (2.43.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (1.26.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith==0.3.18->-r requirements_new.txt (line 18)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith==0.3.18->-r requirements_new.txt (line 18)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.46->-r requirements_new.txt (line 2)) (2.1)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (21.3.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.14.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.17.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (1.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.6.0->-r requirements_new.txt (line 42)) (2.1.3)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab->jupyter==1.1.1->-r requirements_new.txt (line 39)) (2.0.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab->jupyter==1.1.1->-r requirements_new.txt (line 39)) (2.2.0)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (8.27.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.1.6)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook==7.3.3->-r requirements_new.txt (line 40)) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.9.6)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.10)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.18->-r requirements_new.txt (line 17)) (1.12.0)\n",
      "Requirement already satisfied: llama-cloud==0.1.19 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.4->llama-parse==0.6.4.post1->-r requirements_new.txt (line 62)) (0.1.19)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.4->llama-parse==0.6.4.post1->-r requirements_new.txt (line 62)) (4.5.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert->jupyter==1.1.1->-r requirements_new.txt (line 39)) (1.2.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.6.3->-r requirements_new.txt (line 29)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.6.3->-r requirements_new.txt (line 29)) (25.2.10)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (6.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.48b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (3.11.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.20->-r requirements_new.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymongo<5.0.0,>=4.5.0->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (2.8.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.11.23->-r requirements_new.txt (line 61)) (3.0.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r requirements_new.txt (line 29)) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.6.3->-r requirements_new.txt (line 29)) (10.4)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->ragas==0.2.14->-r requirements_new.txt (line 52)) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets->ragas==0.2.14->-r requirements_new.txt (line 52)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/lib/python3.12/site-packages (from datasets->ragas==0.2.14->-r requirements_new.txt (line 52)) (0.70.18)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from ftfy->open-clip-torch==2.31.0->-r requirements_new.txt (line 45)) (0.2.5)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from html5lib->unstructured==0.17.0->-r requirements_new.txt (line 53)) (0.5.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets->jupyter==1.1.1->-r requirements_new.txt (line 39)) (3.6.6)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets->jupyter==1.1.1->-r requirements_new.txt (line 39)) (1.0.0)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-console->jupyter==1.1.1->-r requirements_new.txt (line 39)) (3.0.43)\n",
      "Requirement already satisfied: grpc-gateway-protoc-gen-openapiv2==0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from pinecone-client[grpc]->langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (0.1.0)\n",
      "Requirement already satisfied: lz4>=3.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from pinecone-client[grpc]->langchain-teddynote==0.3.44->-r requirements_new.txt (line 15)) (4.3.2)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured==0.17.0->-r requirements_new.txt (line 53)) (25.1.0)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from unstructured-client->unstructured==0.17.0->-r requirements_new.txt (line 53)) (0.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/anaconda3/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (21.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six==20231228->-r requirements_new.txt (line 34)) (2.21)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.43.2->-r requirements_new.txt (line 38)) (4.0.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (1.48.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (3.17.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (4.8.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2->-r requirements_new.txt (line 38)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2->-r requirements_new.txt (line 38)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.43.2->-r requirements_new.txt (line 38)) (0.10.6)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.1.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (0.1.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->langchain-azure-ai==0.1.2->-r requirements_new.txt (line 14)) (2.8.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/anaconda3/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (2.16.2)\n",
      "Requirement already satisfied: ptyprocess in /opt/anaconda3/lib/python3.12/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (0.7.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.6.3->-r requirements_new.txt (line 29)) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb==0.6.3->-r requirements_new.txt (line 29)) (3.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.8.3)\n",
      "Requirement already satisfied: fqdn in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (20.11.0)\n",
      "Requirement already satisfied: uri-template in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (25.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai==2.1.0->-r requirements_new.txt (line 12)) (0.4.8)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.1.1->-r requirements_new.txt (line 39)) (0.2.2)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook==7.3.3->-r requirements_new.txt (line 40)) (1.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements_new.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbadfe",
   "metadata": {},
   "source": [
    "# PDF Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf9c8c",
   "metadata": {},
   "source": [
    "PDF Loader Rank (The lower, the better)\n",
    "\n",
    "| | PDFMiner | PDFPlumber | PyPDFium2 | PyMuPDF | PyPDF2 |\n",
    "|----------|:---------:|:----------:|:---------:|:-------:|:-----:|\n",
    "| Medical  | 1         | 2          | 3         | 4       | 5     |\n",
    "| Law      | 3         | 1          | 1         | 3       | 5     |\n",
    "| Finance  | 1         | 2          | 2         | 4       | 5     |\n",
    "| Public   | 1         | 1          | 1         | 4       | 5     |\n",
    "| Sum      | 5         | 5          | 7         | 15      | 20    |\n",
    "\n",
    "Source: [AutoRAG Medium Blog](https://velog.io/@autorag/PDF-%ED%95%9C%EA%B8%80-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%B6%94%EC%B6%9C-%EC%8B%A4%ED%97%98#%EC%B4%9D%ED%8F%89)\n",
    "\n",
    "### LlamaParser\n",
    "- Support for a wide range of document formats, including PDF, Word, PowerPoint, and Excel\n",
    "- Advanced extraction capabilities for complex tables and images\n",
    "- Multilingual document processing support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3b989dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cloud.llamaindex.ai/api/parsing/upload \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 9a19f5c3-d2e7-4e1c-ab71-902edc8b687b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/9a19f5c3-d2e7-4e1c-ab71-902edc8b687b \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/9a19f5c3-d2e7-4e1c-ab71-902edc8b687b \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/9a19f5c3-d2e7-4e1c-ab71-902edc8b687b/result/markdown \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents count: 22\n",
      "\n",
      "--- Document 0 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 1, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 1 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 2, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 2 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 3, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 3 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 4, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 4 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 5, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 5 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 6, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 6 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 7, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 7 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 8, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 8 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 9, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 9 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 10, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 10 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 11, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 11 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 12, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 12 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 13, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 13 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 14, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 14 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 15, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 15 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 16, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 16 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 17, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 17 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 18, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 18 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 19, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 19 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 20, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 20 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 21, 'source': 'data/Deepseek-r1.pdf'}\n",
      "\n",
      "--- Document 21 ---\n",
      "{'file_path': 'data/Deepseek-r1.pdf', 'file_name': 'Deepseek-r1.pdf', 'file_type': 'application/pdf', 'file_size': 1312189, 'creation_date': '2025-12-08', 'last_modified_date': '2025-12-05', 'page': 22, 'source': 'data/Deepseek-r1.pdf'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 1. LlamaParse Loader\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",  \n",
    "    num_workers=8,  \n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/Deepseek-r1.pdf\"],\n",
    "    file_extractor=file_extractor,\n",
    ").load_data()\n",
    "\n",
    "documents = [doc.to_langchain_format() for doc in documents]\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    doc.metadata[\"page\"] = idx + 1\n",
    "    doc.metadata[\"source\"] = doc.metadata.get(\n",
    "        \"file_path\", \"data/Deepseek-r1.pdf\"\n",
    "    )\n",
    "\n",
    "print(f\"Loaded documents count: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8936cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 0 ---\n",
      "arXiv:2501.12948v1 [cs.CL] 22 Jan 2025\n",
      "# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      "\n",
      "DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "\n",
      "| DeepSeek-R1 | OpenAI-o1-1217 | DeepSeek-R1-32B | OpenAI-o1-mini | DeepSeek-V3 |            |                    |\n",
      "| ----------- | -------------- | --------------- | -------------- | ----------- | ---------- | ------------------ |\n",
      "| 100         | 96.3           | 96.6            | 97.3           | 96.4        |            |                    |\n",
      "|             | 90.6           | 93.4            | 94.3           | 90.0        | 90.2       |                    |\n",
      "|             | 90.8           | 91.8            | 87.4           | 88.5        | 85.2       |                    |\n",
      "| 80          | 79.8           | 79.2            | 75.7           |             |            |                    |\n",
      "| 72.6        | 71.5           |                 |                |             |            |                    |\n",
      "| (%)         | Percentile     | 63.6            | 62.1           | 60.0        |            |                    |\n",
      "| 60          | 58.7           | 59.1            |                |             |            |                    |\n",
      "|             | 49.2           | 48.9            | 41.6           | 42.0        |            |                    |\n",
      "| 40          | 39.2           |                 | 36.8           |             |            |                    |\n",
      "| 20          |                |                 |                |             |            |                    |\n",
      "| 0           | AIME 2024      | Codeforces      | GPQA Diamond   | MATH-500    | MMLU       | SWE-bench Verified |\n",
      "| (Pass\\@1)   | (Percentile)   | (Pass\\@1)       | (Pass\\@1)      | (Pass\\@1)   | (Resolved) |                    |\n",
      "\n",
      "# Figure 1\n",
      "\n",
      "Benchmark performance of DeepSeek-R1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "\n",
      "# Contents\n",
      "\n",
      "# 1 Introduction\n",
      "\n",
      "# 1.1 Contributions\n",
      "\n",
      "# 1.2 Summary of Evaluation Results\n",
      "\n",
      "# 2 Approach\n",
      "\n",
      "# 2.1 Overview\n",
      "\n",
      "# 2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n",
      "\n",
      "# 2.2.1 Reinforcement Learning Algorithm\n",
      "\n",
      "# 2.2.2 Reward Modeling\n",
      "\n",
      "# 2.2.3 Training Template\n",
      "\n",
      "# 2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "# 2.3 DeepSeek-R1: Reinforcement Learning with Cold Start\n",
      "\n",
      "# 2.3.1 Cold Start\n",
      "\n",
      "# 2.3.2 Reasoning-oriented Reinforcement Learning\n",
      "\n",
      "# 2.3.3 Rejection Sampling and Supervised Fine-Tuning\n",
      "\n",
      "# 2.3.4 Reinforcement Learning for all Scenarios\n",
      "\n",
      "# 2.4 Distillation: Empower Small Models with Reasoning Capability\n",
      "\n",
      "# 3 Experiment\n",
      "\n",
      "# 3.1 DeepSeek-R1 Evaluation\n",
      "\n",
      "# 3.2 Distilled Model Evaluation\n",
      "\n",
      "# 4 Discussion\n",
      "\n",
      "# 4.1 Distillation v.s. Reinforcement Learning\n",
      "\n",
      "# 4.2 Unsuccessful Attempts\n",
      "\n",
      "# 5 Conclusion, Limitations, and Future Work\n",
      "\n",
      "# A Contributions and Acknowledgments\n",
      "\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "# 1. Introduction\n",
      "\n",
      "In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI).\n",
      "\n",
      "Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAIs o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAIs o1 series models.\n",
      "\n",
      "In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\n",
      "\n",
      "During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.\n",
      "\n",
      "However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\n",
      "\n",
      "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\n",
      "\n",
      "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.\n",
      "\n",
      "\n",
      "--- Document 3 ---\n",
      "\n",
      "\n",
      "\n",
      "# 1.1. Contributions\n",
      "\n",
      "# Post-Training: Large-Scale Reinforcement Learning on the Base Model\n",
      "\n",
      "- We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n",
      "- We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.\n",
      "\n",
      "# Distillation: Smaller Models Can Be Powerful Too\n",
      "\n",
      "- We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\n",
      "- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n",
      "\n",
      "# 1.2. Summary of Evaluation Results\n",
      "\n",
      "- Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\n",
      "- Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 4 ---\n",
      "\n",
      "\n",
      " Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n",
      "\n",
      "# 2. Approach\n",
      "\n",
      "# 2.1. Overview\n",
      "\n",
      "Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\n",
      "\n",
      "# 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n",
      "\n",
      "Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.\n",
      "\n",
      "# 2.2.1. Reinforcement Learning Algorithm\n",
      "\n",
      "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question , GRPO samples a group of outputs {1, 2,    , } from the old policy  and then optimizes the policy model  by maximizing the following objective:\n",
      "\n",
      "JGRPO() = E[   (), {}    (|)]\n",
      "\n",
      "1          min  ( |)          , clip   ( |)    , 1  , 1 +     DKL  ||ref ,\n",
      "\n",
      " =1         ( |)                 ( |)\n",
      "\n",
      "DKL  ||ref = ref(( |)  log ref ( |)  1,\n",
      "\n",
      "  |)     ( |)\n",
      "\n",
      "where  and  are hyper-parameters, and  is the advantage, computed using a group of rewards {1, 2, . . . , } corresponding to the outputs within each group:\n",
      "\n",
      " =                  sm({1, 2,    , }).\n",
      "\n",
      " ({1, 2,     , })\n",
      "\n",
      "\n",
      "--- Document 5 ---\n",
      "\n",
      "\n",
      "\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:\n",
      "\n",
      "|   |\n",
      "| - |\n",
      "\n",
      "# 2.2.2. Reward Modeling\n",
      "\n",
      "The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:\n",
      "\n",
      "- Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
      "- Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between <think> and </think> tags.\n",
      "\n",
      "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n",
      "\n",
      "# 2.2.3. Training Template\n",
      "\n",
      "To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process.\n",
      "\n",
      "# 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.\n",
      "\n",
      "|   |\n",
      "| - |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 6 ---\n",
      "\n",
      "\n",
      "# Model\n",
      "\n",
      "|                  |         | AIME 2024 | MATH-500 | GPQA    | LiveCode | CodeForces |\n",
      "| ---------------- | ------- | --------- | -------- | ------- | -------- | ---------- |\n",
      "|                  | pass\\@1 | cons\\@64  | pass\\@1  | pass\\@1 | pass\\@1  | rating     |\n",
      "| OpenAI-o1-mini   | 63.6    | 80.0      | 90.0     | 60.0    | 53.8     | 1820       |\n",
      "| OpenAI-o1-0912   | 74.4    | 83.3      | 94.8     | 77.3    | 63.4     | 1843       |\n",
      "| DeepSeek-R1-Zero | 71.0    | 86.7      | 95.9     | 73.3    | 50.0     | 1444       |\n",
      "\n",
      "# Table 2\n",
      "\n",
      "Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.\n",
      "\n",
      "| 0008         | 0009                    | Sdegs |\n",
      "| ------------ | ----------------------- | ----- |\n",
      "| 000t         | 2000                    | 0     |\n",
      "| 9su0-7I60-00 | TSsed-Z160-1O           |       |\n",
      "| 9Isoo-0J-IJ  | Tssed-oJz-T             |       |\n",
      "|              | '0                     |       |\n",
      "|              | 0                       |       |\n",
      "|              | 0020                    |       |\n",
      "|              | 90                      |       |\n",
      "|              | L'O                     |       |\n",
      "|              | 8'0                     |       |\n",
      "|              | Deee Cen enn r caem-r-P |       |\n",
      "|              | 6'0                     |       |\n",
      "\n",
      "# Figure 2\n",
      "\n",
      "AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.\n",
      "\n",
      "DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n",
      "\n",
      "# Self-evolution Process of DeepSeek-R1-Zero\n",
      "\n",
      "The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n",
      "\n",
      "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 7 ---\n",
      "\n",
      "\n",
      "# DeepSeek-R1-Zero average length per response during training\n",
      "\n",
      "|                 | 0     | 2000 | 4000 | 6000 | 8000 |\n",
      "| --------------- | ----- | ---- | ---- | ---- | ---- |\n",
      "| Response Length | 12000 |      |      |      |      |\n",
      "|                 | 10000 |      |      |      |      |\n",
      "|                 | 8000  |      |      |      |      |\n",
      "|                 | 6000  |      |      |      |      |\n",
      "|                 | 4000  |      |      |      |      |\n",
      "|                 | 2000  |      |      |      |      |\n",
      "|                 | 0     |      |      |      |      |\n",
      "\n",
      "Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n",
      "\n",
      "DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.\n",
      "\n",
      "One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n",
      "\n",
      "# Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
      "\n",
      "This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 8 ---\n",
      "\n",
      "\n",
      "# Question:\n",
      "\n",
      "If  > 1, then the sum of the real solutions of     +  =  is equal to\n",
      "\n",
      "# Response:\n",
      "\n",
      "<think>      </think>\n",
      "\n",
      "To solve the equation        +  = , lets start by squaring both   \n",
      "\n",
      "     +  2 = 2 =    +  = 2.\n",
      "\n",
      "Rearrange to isolate the inner square root term:\n",
      "\n",
      "(  2)2 =  +  = 2  22 + (2)2 =  +  = 4  22   + (2  ) = 0\n",
      "\n",
      "Wait, wait. Wait. Thats an aha moment I can flag here.\n",
      "\n",
      "Lets reevaluate this step-by-step to identify if the correct sum can be   \n",
      "\n",
      "We started with the equation:\n",
      "\n",
      "   +  = \n",
      "\n",
      "First, lets square both sides:\n",
      "\n",
      "   +  = 2    =  +  =   2\n",
      "\n",
      "Next, I could square both sides again, treating the equation:   \n",
      "\n",
      "# Table 3\n",
      "\n",
      "| An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.\n",
      "\n",
      "# Drawback of DeepSeek-R1-Zero\n",
      "\n",
      "Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\n",
      "\n",
      "# 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\n",
      "\n",
      "Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\n",
      "\n",
      "# 2.3.1. Cold Start\n",
      "\n",
      "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators.\n",
      "\n",
      "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 9 ---\n",
      "\n",
      "\n",
      "# 2.3.2. Reasoning-oriented Reinforcement Learning\n",
      "\n",
      "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.\n",
      "\n",
      "# 2.3.3. Rejection Sampling and Supervised Fine-Tuning\n",
      "\n",
      "When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.\n",
      "\n",
      "Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 10 ---\n",
      "\n",
      "\n",
      "Non-Reasoning data\n",
      "\n",
      "For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as hello we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.\n",
      "\n",
      "# 2.3.4. Reinforcement Learning for all Scenarios\n",
      "\n",
      "To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the models helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n",
      "\n",
      "# 2.4. Distillation: Empower Small Models with Reasoning Capability\n",
      "\n",
      "To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n",
      "\n",
      "For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.\n",
      "\n",
      "# 3. Experiment\n",
      "\n",
      "Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 11 ---\n",
      "\n",
      "\n",
      "2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08  2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n",
      "\n",
      "# Evaluation Prompts\n",
      "\n",
      "Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.\n",
      "\n",
      "# Baselines\n",
      "\n",
      "We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).\n",
      "\n",
      "# Evaluation Setup\n",
      "\n",
      "We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@ evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top- value of 0.95 to generate  responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as\n",
      "\n",
      "pass@1 = 1 / k *  (pi), where pi denotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n",
      "\n",
      "1https://aider.chat\n",
      "\n",
      "2https://codeforces.com\n",
      "\n",
      "3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 12 ---\n",
      "\n",
      "# 3.1. DeepSeek-R1 Evaluation\n",
      "\n",
      "| Benchmark (Metric)              | Claude-3.5-             | GPT-4o | DeepSeek | OpenAI | OpenAI | DeepSeek |      |\n",
      "| ------------------------------- | ----------------------- | ------ | -------- | ------ | ------ | -------- | ---- |\n",
      "| Architecture                    | -                       | -      | MoE      | -      | -      | MoE      |      |\n",
      "| # Activated Params              | -                       | -      | 37B      | -      | -      | 37B      |      |\n",
      "| # Total Params                  | -                       | -      | 671B     | -      | -      | 671B     |      |\n",
      "| MMLU (Pass\\@1)                  | 88.3                    | 87.2   | 88.5     | 85.2   | 91.8   | 90.8     |      |\n",
      "| MMLU-Redux (EM)                 | 88.9                    | 88.0   | 89.1     | 86.7   | -      | 92.9     |      |\n",
      "| MMLU-Pro (EM)                   | 78.0                    | 72.6   | 75.9     | 80.3   | -      | 84.0     |      |\n",
      "| DROP (3-shot F1)                | 88.3                    | 83.7   | 91.6     | 83.9   | 90.2   | 92.2     |      |\n",
      "| English IF-Eval (Prompt Strict) | 86.5                    | 84.3   | 86.1     | 84.8   | -      | 83.3     |      |\n",
      "| GPQA Diamond (Pass\\@1)          | 65.0                    | 49.9   | 59.1     | 60.0   | 75.7   | 71.5     |      |\n",
      "| SimpleQA (Correct)              | 28.4                    | 38.2   | 24.9     | 7.0    | 47.0   | 30.1     |      |\n",
      "| FRAMES (Acc.)                   | 72.5                    | 80.5   | 73.3     | 76.9   | -      | 82.5     |      |\n",
      "| AlpacaEval2.0 (LC-winrate)      | 52.0                    | 51.1   | 70.0     | 57.8   | -      | 87.6     |      |\n",
      "| ArenaHard (GPT-4-1106)          | 85.2                    | 80.4   | 85.5     | 92.0   | -      | 92.3     |      |\n",
      "| LiveCodeBench (Pass\\@1-COT)     | 38.9                    | 32.9   | 36.2     | 53.8   | 63.4   | 65.9     |      |\n",
      "| Code                            | Codeforces (Percentile) | 20.3   | 23.6     | 58.7   | 93.4   | 96.6     | 96.3 |\n",
      "| Codeforces (Rating)             | 717                     | 759    | 1134     | 1820   | 2061   | 2029     |      |\n",
      "| SWE Verified (Resolved)         | 50.8                    | 38.8   | 42.0     | 41.6   | 48.9   | 49.2     |      |\n",
      "| Aider-Polyglot (Acc.)           | 45.3                    | 16.0   | 49.6     | 32.9   | 61.7   | 53.3     |      |\n",
      "| AIME 2024 (Pass\\@1)             | 16.0                    | 9.3    | 39.2     | 63.6   | 79.2   | 79.8     |      |\n",
      "| Math                            | MATH-500 (Pass\\@1)      | 78.3   | 74.6     | 90.2   | 90.0   | 96.4     | 97.3 |\n",
      "| CNMO 2024 (Pass\\@1)             | 13.1                    | 10.8   | 43.2     | 67.6   | -      | 78.8     |      |\n",
      "| CLUEWSC (EM)                    | 85.4                    | 87.9   | 90.9     | 89.9   | -      | 92.8     |      |\n",
      "| Chinese C-Eval (EM)             | 76.7                    | 76.0   | 86.5     | 68.9   | -      | 91.8     |      |\n",
      "| C-SimpleQA (Correct)            | 55.4                    | 58.7   | 68.0     | 40.3   | -      | 63.7     |      |\n",
      "\n",
      "Table 4 | Comparison between DeepSeek-R1 and other representative models.\n",
      "\n",
      "For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%.\n",
      "\n",
      "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\n",
      "\n",
      "\n",
      "--- Document 13 ---\n",
      "\n",
      "\n",
      "DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n",
      "\n",
      "On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n",
      "\n",
      "# 3.2. Distilled Model Evaluation\n",
      "\n",
      "| Model                         | AIME 2024 |      | MATH-500 | GPQA | LiveCode Diamond | CodeForces Bench |\n",
      "| ----------------------------- | --------- | ---- | -------- | ---- | ---------------- | ---------------- |\n",
      "| GPT-4o-0513                   | 9.3       | 13.4 | 74.6     | 49.9 | 32.9             | 759              |\n",
      "| Claude-3.5-Sonnet-1022        | 16.0      | 26.7 | 78.3     | 65.0 | 38.9             | 717              |\n",
      "| OpenAI-o1-mini                | 63.6      | 80.0 | 90.0     | 60.0 | 53.8             | 1820             |\n",
      "| QwQ-32B-Preview               | 50.0      | 60.0 | 90.6     | 54.5 | 41.9             | 1316             |\n",
      "| DeepSeek-R1-Distill-Qwen-1.5B | 28.9      | 52.7 | 83.9     | 33.8 | 16.9             | 954              |\n",
      "| DeepSeek-R1-Distill-Qwen-7B   | 55.5      | 83.3 | 92.8     | 49.1 | 37.6             | 1189             |\n",
      "| DeepSeek-R1-Distill-Qwen-14B  | 69.7      | 80.0 | 93.9     | 59.1 | 53.1             | 1481             |\n",
      "| DeepSeek-R1-Distill-Qwen-32B  | 72.6      | 83.3 | 94.3     | 62.1 | 57.2             | 1691             |\n",
      "| DeepSeek-R1-Distill-Llama-8B  | 50.4      | 80.0 | 89.1     | 49.0 | 39.6             | 1205             |\n",
      "| DeepSeek-R1-Distill-Llama-70B | 70.0      | 86.7 | 94.5     | 65.2 | 57.5             | 1633             |\n",
      "\n",
      "Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.\n",
      "\n",
      "As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n",
      "\n",
      "# 4. Discussion\n",
      "\n",
      "# 4.1. Distillation v.s. Reinforcement Learning\n",
      "\n",
      "In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n",
      "\n",
      "To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 14 ---\n",
      "\n",
      "AIME 2024\n",
      "# Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\n",
      "\n",
      "| Model                        | pass\\@1 | cons\\@64 | pass\\@1 | pass\\@1 | pass\\@1 |\n",
      "| ---------------------------- | ------- | -------- | ------- | ------- | ------- |\n",
      "| QwQ-32B-Preview              | 50.0    | 60.0     | 90.6    | 54.5    | 41.9    |\n",
      "| DeepSeek-R1-Zero-Qwen-32B    | 47.0    | 60.0     | 91.6    | 55.0    | 40.2    |\n",
      "| DeepSeek-R1-Distill-Qwen-32B | 72.6    | 83.3     | 94.3    | 62.1    | 57.2    |\n",
      "\n",
      "RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning.\n",
      "\n",
      "# 4.2. Unsuccessful Attempts\n",
      "\n",
      "In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n",
      "\n",
      "# Process Reward Model (PRM)\n",
      "\n",
      "PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n",
      "\n",
      "# Monte Carlo Tree Search (MCTS)\n",
      "\n",
      "Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 15 ---\n",
      "\n",
      "\n",
      "exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGos core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.\n",
      "\n",
      "In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge.\n",
      "\n",
      "# 5. Conclusion, Limitations, and Future Work\n",
      "\n",
      "In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n",
      "\n",
      "We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n",
      "\n",
      "In the future, we plan to invest in research across the following directions for DeepSeek-R1.\n",
      "\n",
      "- General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      "- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n",
      "- Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n",
      "- Software Engineering Tasks: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 16 ---\n",
      "\n",
      "\n",
      "# References\n",
      "\n",
      "- AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\n",
      "- Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet.\n",
      "- M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\n",
      "- A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n",
      "- Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\n",
      "- X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https://arxiv.org/abs/2309.17179.\n",
      "- L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760.\n",
      "- A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.org/10.48550/arXiv.2406.04127.\n",
      "- Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024.\n",
      "- Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.\n",
      "- D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n",
      "- Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n",
      "- N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 17 ---\n",
      "\n",
      "\n",
      "# References\n",
      "\n",
      "S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.48550/arXiv.2409.12941.\n",
      "\n",
      "A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\n",
      "\n",
      "H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.\n",
      "\n",
      "T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024.\n",
      "\n",
      "H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n",
      "\n",
      "B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval.\n",
      "\n",
      "MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime.\n",
      "\n",
      "OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\n",
      "\n",
      "OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learning-to-reason-with-llms/.\n",
      "\n",
      "OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing-simpleqa/.\n",
      "\n",
      "OpenAI. Introducing SWE-bench verified were releasing a human-validated subset of swe-bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench-verified/.\n",
      "\n",
      "Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm.github.io/blog/qwq-32b-preview/.\n",
      "\n",
      "Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/blog/qwen2.5.\n",
      "\n",
      "D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&#x26;a benchmark. arXiv preprint arXiv:2311.12022, 2023.\n",
      "\n",
      "Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n",
      "\n",
      "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 18 ---\n",
      "\n",
      "\n",
      "D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\n",
      "\n",
      "C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314.\n",
      "\n",
      "T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\n",
      "\n",
      "J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\n",
      "\n",
      "P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023.\n",
      "\n",
      "X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
      "\n",
      "Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URL https://doi.org/10.48550/arXiv.2406.01574.\n",
      "\n",
      "C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\n",
      "\n",
      "H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.\n",
      "\n",
      "J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 19 ---\n",
      "\n",
      "Appendix\n",
      "\n",
      "# A. Contributions and Acknowledgments\n",
      "\n",
      "| Core Contributors | Hui Li         |\n",
      "| ----------------- | -------------- |\n",
      "|                   | Daya Guo       |\n",
      "|                   | Jianzhong Guo  |\n",
      "|                   | Dejian Yang    |\n",
      "|                   | Jiashi Li      |\n",
      "|                   | Haowei Zhang   |\n",
      "|                   | Jingchang Chen |\n",
      "|                   | Junxiao Song   |\n",
      "|                   | Jingyang Yuan  |\n",
      "|                   | Ruoyu Zhang    |\n",
      "|                   | Jinhao Tu      |\n",
      "|                   | Runxin Xu      |\n",
      "|                   | Junjie Qiu     |\n",
      "|                   | Qihao Zhu      |\n",
      "|                   | Junlong Li     |\n",
      "|                   | Shirong Ma     |\n",
      "|                   | J.L. Cai       |\n",
      "|                   | Peiyi Wang     |\n",
      "|                   | Jiaqi Ni       |\n",
      "|                   | Xiao Bi        |\n",
      "|                   | Jian Liang     |\n",
      "|                   | Xiaokang Zhang |\n",
      "|                   | Jin Chen       |\n",
      "|                   | Xingkai Yu     |\n",
      "|                   | Kai Dong       |\n",
      "|                   | Yu Wu          |\n",
      "|                   | Kai Hu\\*       |\n",
      "|                   | Z.F. Wu        |\n",
      "|                   | Kaichao You    |\n",
      "|                   | Zhibin Gou     |\n",
      "|                   | Kaige Gao      |\n",
      "|                   | Zhihong Shao   |\n",
      "|                   | Kang Guan      |\n",
      "|                   | Zhuoshu Li     |\n",
      "|                   | Kexin Huang    |\n",
      "|                   | Ziyi Gao       |\n",
      "|                   | Kuai Yu        |\n",
      "|                   | Lean Wang      |\n",
      "\n",
      "| Contributors | Lecong Zhang    |\n",
      "| ------------ | --------------- |\n",
      "|              | Aixin Liu       |\n",
      "|              | Liang Zhao      |\n",
      "|              | Bing Xue        |\n",
      "|              | Litong Wang     |\n",
      "|              | Bingxuan Wang   |\n",
      "|              | Liyue Zhang     |\n",
      "|              | Bochao Wu       |\n",
      "|              | Lei Xu          |\n",
      "|              | Bei Feng        |\n",
      "|              | Leyi Xia        |\n",
      "|              | Chengda Lu      |\n",
      "|              | Mingchuan Zhang |\n",
      "|              | Chenggang Zhao  |\n",
      "|              | Minghua Zhang   |\n",
      "|              | Chengqi Deng    |\n",
      "|              | Minghui Tang    |\n",
      "|              | Chong Ruan      |\n",
      "|              | Mingxu Zhou     |\n",
      "|              | Damai Dai       |\n",
      "|              | Meng Li         |\n",
      "|              | Deli Chen       |\n",
      "|              | Miaojun Wang    |\n",
      "|              | Dongjie Ji      |\n",
      "|              | Mingming Li     |\n",
      "|              | Erhang Li       |\n",
      "|              | Ning Tian       |\n",
      "|              | Fangyun Lin     |\n",
      "|              | Panpan Huang    |\n",
      "|              | Fucong Dai      |\n",
      "|              | Peng Zhang      |\n",
      "|              | Fuli Luo\\*      |\n",
      "|              | Qiancheng Wang  |\n",
      "|              | Guangbo Hao     |\n",
      "|              | Qinyu Chen      |\n",
      "|              | Guanting Chen   |\n",
      "|              | Qiushi Du       |\n",
      "|              | Guowei Li       |\n",
      "|              | Ruiqi Ge\\*      |\n",
      "|              | H. Zhang        |\n",
      "|              | Ruisong Zhang   |\n",
      "|              | Hanwei Xu       |\n",
      "|              | Ruizhe Pan      |\n",
      "|              | Honghui Ding    |\n",
      "|              | Runji Wang      |\n",
      "|              | Huazuo Gao      |\n",
      "|              | R.J. Chen       |\n",
      "|              | Hui Qu          |\n",
      "|              | R.L. Jin        |\n",
      "\n",
      "20\n",
      "\n",
      "\n",
      "--- Document 20 ---\n",
      "\n",
      "\n",
      "# Authors\n",
      "\n",
      "| Ruyi Chen      | Y.X. Wei       |\n",
      "| -------------- | -------------- |\n",
      "| Shanghao Lu    | Yang Zhang     |\n",
      "| Shangyan Zhou  | Yanhong Xu     |\n",
      "| Shanhuang Chen | Yao Li         |\n",
      "| Shengfeng Ye   | Yao Zhao       |\n",
      "| Shiyu Wang     | Yaofeng Sun    |\n",
      "| Shuiping Yu    | Yaohui Wang    |\n",
      "| Shunfeng Zhou  | Yi Yu          |\n",
      "| Shuting Pan    | Yichao Zhang   |\n",
      "| S.S. Li        | Yifan Shi      |\n",
      "| Shuang Zhou    | Yiliang Xiong  |\n",
      "| Shaoqing Wu    | Ying He        |\n",
      "| Shengfeng Ye   | Yishi Piao     |\n",
      "| Tao Yun        | Yisong Wang    |\n",
      "| Tian Pei       | Yixuan Tan     |\n",
      "| Tianyu Sun     | Yiyang Ma\\*    |\n",
      "| T. Wang        | Yiyuan Liu     |\n",
      "| Wangding Zeng  | Yongqiang Guo  |\n",
      "| Wen Liu        | Yuan Ou        |\n",
      "| Wenfeng Liang  | Yuduan Wang    |\n",
      "| Wenjun Gao     | Yue Gong       |\n",
      "| Wenqin Yu\\*    | Yuheng Zou     |\n",
      "| Wentao Zhang   | Yujia He       |\n",
      "| W\\.L. Xiao     | Yunfan Xiong   |\n",
      "| Wei An         | Yuxiang Luo    |\n",
      "| Xiaodong Liu   | Yuxiang You    |\n",
      "| Xiaohan Wang   | Yuxuan Liu     |\n",
      "| Xiaokang Chen  | Yuyang Zhou    |\n",
      "| Xiaotao Nie    | Y.X. Zhu       |\n",
      "| Xin Cheng      | Yanping Huang  |\n",
      "| Xin Liu        | Yaohui Li      |\n",
      "| Xin Xie        | Yi Zheng       |\n",
      "| Xingchao Liu   | Yuchen Zhu     |\n",
      "| Xinyu Yang     | Yunxian Ma     |\n",
      "| Xinyuan Li     | Ying Tang      |\n",
      "| Xuecheng Su    | Yukun Zha      |\n",
      "| Xuheng Lin     | Yuting Yan     |\n",
      "| X.Q. Li        | Z.Z. Ren       |\n",
      "| Xiangyue Jin   | Zehui Ren      |\n",
      "| Xiaojin Shen   | Zhangli Sha    |\n",
      "| Xiaosha Chen   | Zhe Fu         |\n",
      "| Xiaowen Sun    | Zhean Xu       |\n",
      "| Xiaoxiang Wang | Zhenda Xie     |\n",
      "| Xinnan Song    | Zhengyan Zhang |\n",
      "| Xinyi Zhou     | Zhewen Hao     |\n",
      "| Xianzu Wang    | Zhicheng Ma    |\n",
      "| Xinxia Shan    | Zhigang Yan    |\n",
      "| Y.K. Li        | Zhiyu Wu       |\n",
      "| Y.Q. Wang      | Zihui Gu       |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 21 ---\n",
      "\n",
      "\n",
      "# Authors\n",
      "\n",
      "Zijia Zhu      Zhen Huang\n",
      "\n",
      "Zijun Liu*     Zhipeng Xu\n",
      "\n",
      "Zilin Li       Zhongyu Zhang\n",
      "\n",
      "Ziwei Xie      Zhen Zhang\n",
      "\n",
      "Ziyang Song\n",
      "\n",
      "Zizheng Pan\n",
      "\n",
      "Within each role, authors are listed alphabetically by the first name. Names marked with * denote individuals who have departed from our team.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(22, len(documents))):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(documents[i].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38c628b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Remove Reference and Appendix\n",
    "documents = documents[:16]\n",
    "\n",
    "# Remove think and answer black to avoid CoT confusion.\n",
    "def remove_think_and_answer_blocks(text):\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"<answer>.*?</answer>\", \"\", text, flags=re.DOTALL)\n",
    "    return text\n",
    "\n",
    "for doc in documents:\n",
    "    doc.page_content = remove_think_and_answer_blocks(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84ee9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 0 ---\n",
      "arXiv:2501.12948v1 [cs.CL] 22 Jan 2025\n",
      "# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      "\n",
      "DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "\n",
      "| DeepSeek-R1 | OpenAI-o1-1217 | DeepSeek-R1-32B | OpenAI-o1-mini | DeepSeek-V3 |            |                    |\n",
      "| ----------- | -------------- | --------------- | -------------- | ----------- | ---------- | ------------------ |\n",
      "| 100         | 96.3           | 96.6            | 97.3           | 96.4        |            |                    |\n",
      "|             | 90.6           | 93.4            | 94.3           | 90.0        | 90.2       |                    |\n",
      "|             | 90.8           | 91.8            | 87.4           | 88.5        | 85.2       |                    |\n",
      "| 80          | 79.8           | 79.2            | 75.7           |             |            |                    |\n",
      "| 72.6        | 71.5           |                 |                |             |            |                    |\n",
      "| (%)         | Percentile     | 63.6            | 62.1           | 60.0        |            |                    |\n",
      "| 60          | 58.7           | 59.1            |                |             |            |                    |\n",
      "|             | 49.2           | 48.9            | 41.6           | 42.0        |            |                    |\n",
      "| 40          | 39.2           |                 | 36.8           |             |            |                    |\n",
      "| 20          |                |                 |                |             |            |                    |\n",
      "| 0           | AIME 2024      | Codeforces      | GPQA Diamond   | MATH-500    | MMLU       | SWE-bench Verified |\n",
      "| (Pass\\@1)   | (Percentile)   | (Pass\\@1)       | (Pass\\@1)      | (Pass\\@1)   | (Resolved) |                    |\n",
      "\n",
      "# Figure 1\n",
      "\n",
      "Benchmark performance of DeepSeek-R1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "\n",
      "# Contents\n",
      "\n",
      "# 1 Introduction\n",
      "\n",
      "# 1.1 Contributions\n",
      "\n",
      "# 1.2 Summary of Evaluation Results\n",
      "\n",
      "# 2 Approach\n",
      "\n",
      "# 2.1 Overview\n",
      "\n",
      "# 2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n",
      "\n",
      "# 2.2.1 Reinforcement Learning Algorithm\n",
      "\n",
      "# 2.2.2 Reward Modeling\n",
      "\n",
      "# 2.2.3 Training Template\n",
      "\n",
      "# 2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "# 2.3 DeepSeek-R1: Reinforcement Learning with Cold Start\n",
      "\n",
      "# 2.3.1 Cold Start\n",
      "\n",
      "# 2.3.2 Reasoning-oriented Reinforcement Learning\n",
      "\n",
      "# 2.3.3 Rejection Sampling and Supervised Fine-Tuning\n",
      "\n",
      "# 2.3.4 Reinforcement Learning for all Scenarios\n",
      "\n",
      "# 2.4 Distillation: Empower Small Models with Reasoning Capability\n",
      "\n",
      "# 3 Experiment\n",
      "\n",
      "# 3.1 DeepSeek-R1 Evaluation\n",
      "\n",
      "# 3.2 Distilled Model Evaluation\n",
      "\n",
      "# 4 Discussion\n",
      "\n",
      "# 4.1 Distillation v.s. Reinforcement Learning\n",
      "\n",
      "# 4.2 Unsuccessful Attempts\n",
      "\n",
      "# 5 Conclusion, Limitations, and Future Work\n",
      "\n",
      "# A Contributions and Acknowledgments\n",
      "\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "# 1. Introduction\n",
      "\n",
      "In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI).\n",
      "\n",
      "Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAIs o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAIs o1 series models.\n",
      "\n",
      "In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\n",
      "\n",
      "During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.\n",
      "\n",
      "However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\n",
      "\n",
      "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\n",
      "\n",
      "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.\n",
      "\n",
      "\n",
      "--- Document 3 ---\n",
      "\n",
      "\n",
      "\n",
      "# 1.1. Contributions\n",
      "\n",
      "# Post-Training: Large-Scale Reinforcement Learning on the Base Model\n",
      "\n",
      "- We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n",
      "- We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.\n",
      "\n",
      "# Distillation: Smaller Models Can Be Powerful Too\n",
      "\n",
      "- We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\n",
      "- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n",
      "\n",
      "# 1.2. Summary of Evaluation Results\n",
      "\n",
      "- Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\n",
      "- Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 4 ---\n",
      "\n",
      "\n",
      " Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n",
      "\n",
      "# 2. Approach\n",
      "\n",
      "# 2.1. Overview\n",
      "\n",
      "Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\n",
      "\n",
      "# 2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\n",
      "\n",
      "Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.\n",
      "\n",
      "# 2.2.1. Reinforcement Learning Algorithm\n",
      "\n",
      "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question , GRPO samples a group of outputs {1, 2,    , } from the old policy  and then optimizes the policy model  by maximizing the following objective:\n",
      "\n",
      "JGRPO() = E[   (), {}    (|)]\n",
      "\n",
      "1          min  ( |)          , clip   ( |)    , 1  , 1 +     DKL  ||ref ,\n",
      "\n",
      " =1         ( |)                 ( |)\n",
      "\n",
      "DKL  ||ref = ref(( |)  log ref ( |)  1,\n",
      "\n",
      "  |)     ( |)\n",
      "\n",
      "where  and  are hyper-parameters, and  is the advantage, computed using a group of rewards {1, 2, . . . , } corresponding to the outputs within each group:\n",
      "\n",
      " =                  sm({1, 2,    , }).\n",
      "\n",
      " ({1, 2,     , })\n",
      "\n",
      "\n",
      "--- Document 5 ---\n",
      "\n",
      "\n",
      "\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within  and  tags, respectively, i.e.,  . User: prompt. Assistant:\n",
      "\n",
      "|   |\n",
      "| - |\n",
      "\n",
      "# 2.2.2. Reward Modeling\n",
      "\n",
      "The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:\n",
      "\n",
      "- Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
      "- Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between  tags.\n",
      "\n",
      "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n",
      "\n",
      "# 2.2.3. Training Template\n",
      "\n",
      "To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process.\n",
      "\n",
      "# 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.\n",
      "\n",
      "|   |\n",
      "| - |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 6 ---\n",
      "\n",
      "\n",
      "# Model\n",
      "\n",
      "|                  |         | AIME 2024 | MATH-500 | GPQA    | LiveCode | CodeForces |\n",
      "| ---------------- | ------- | --------- | -------- | ------- | -------- | ---------- |\n",
      "|                  | pass\\@1 | cons\\@64  | pass\\@1  | pass\\@1 | pass\\@1  | rating     |\n",
      "| OpenAI-o1-mini   | 63.6    | 80.0      | 90.0     | 60.0    | 53.8     | 1820       |\n",
      "| OpenAI-o1-0912   | 74.4    | 83.3      | 94.8     | 77.3    | 63.4     | 1843       |\n",
      "| DeepSeek-R1-Zero | 71.0    | 86.7      | 95.9     | 73.3    | 50.0     | 1444       |\n",
      "\n",
      "# Table 2\n",
      "\n",
      "Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.\n",
      "\n",
      "| 0008         | 0009                    | Sdegs |\n",
      "| ------------ | ----------------------- | ----- |\n",
      "| 000t         | 2000                    | 0     |\n",
      "| 9su0-7I60-00 | TSsed-Z160-1O           |       |\n",
      "| 9Isoo-0J-IJ  | Tssed-oJz-T             |       |\n",
      "|              | '0                     |       |\n",
      "|              | 0                       |       |\n",
      "|              | 0020                    |       |\n",
      "|              | 90                      |       |\n",
      "|              | L'O                     |       |\n",
      "|              | 8'0                     |       |\n",
      "|              | Deee Cen enn r caem-r-P |       |\n",
      "|              | 6'0                     |       |\n",
      "\n",
      "# Figure 2\n",
      "\n",
      "AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.\n",
      "\n",
      "DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n",
      "\n",
      "# Self-evolution Process of DeepSeek-R1-Zero\n",
      "\n",
      "The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n",
      "\n",
      "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 7 ---\n",
      "\n",
      "\n",
      "# DeepSeek-R1-Zero average length per response during training\n",
      "\n",
      "|                 | 0     | 2000 | 4000 | 6000 | 8000 |\n",
      "| --------------- | ----- | ---- | ---- | ---- | ---- |\n",
      "| Response Length | 12000 |      |      |      |      |\n",
      "|                 | 10000 |      |      |      |      |\n",
      "|                 | 8000  |      |      |      |      |\n",
      "|                 | 6000  |      |      |      |      |\n",
      "|                 | 4000  |      |      |      |      |\n",
      "|                 | 2000  |      |      |      |      |\n",
      "|                 | 0     |      |      |      |      |\n",
      "\n",
      "Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n",
      "\n",
      "DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.\n",
      "\n",
      "One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n",
      "\n",
      "# Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
      "\n",
      "This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 8 ---\n",
      "\n",
      "\n",
      "# Question:\n",
      "\n",
      "If  > 1, then the sum of the real solutions of     +  =  is equal to\n",
      "\n",
      "# Response:\n",
      "\n",
      "\n",
      "\n",
      "To solve the equation        +  = , lets start by squaring both   \n",
      "\n",
      "     +  2 = 2 =    +  = 2.\n",
      "\n",
      "Rearrange to isolate the inner square root term:\n",
      "\n",
      "(  2)2 =  +  = 2  22 + (2)2 =  +  = 4  22   + (2  ) = 0\n",
      "\n",
      "Wait, wait. Wait. Thats an aha moment I can flag here.\n",
      "\n",
      "Lets reevaluate this step-by-step to identify if the correct sum can be   \n",
      "\n",
      "We started with the equation:\n",
      "\n",
      "   +  = \n",
      "\n",
      "First, lets square both sides:\n",
      "\n",
      "   +  = 2    =  +  =   2\n",
      "\n",
      "Next, I could square both sides again, treating the equation:   \n",
      "\n",
      "# Table 3\n",
      "\n",
      "| An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.\n",
      "\n",
      "# Drawback of DeepSeek-R1-Zero\n",
      "\n",
      "Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\n",
      "\n",
      "# 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\n",
      "\n",
      "Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\n",
      "\n",
      "# 2.3.1. Cold Start\n",
      "\n",
      "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators.\n",
      "\n",
      "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 9 ---\n",
      "\n",
      "\n",
      "# 2.3.2. Reasoning-oriented Reinforcement Learning\n",
      "\n",
      "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.\n",
      "\n",
      "# 2.3.3. Rejection Sampling and Supervised Fine-Tuning\n",
      "\n",
      "When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.\n",
      "\n",
      "Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 10 ---\n",
      "\n",
      "\n",
      "Non-Reasoning data\n",
      "\n",
      "For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as hello we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.\n",
      "\n",
      "# 2.3.4. Reinforcement Learning for all Scenarios\n",
      "\n",
      "To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the models helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n",
      "\n",
      "# 2.4. Distillation: Empower Small Models with Reasoning Capability\n",
      "\n",
      "To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n",
      "\n",
      "For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.\n",
      "\n",
      "# 3. Experiment\n",
      "\n",
      "Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 11 ---\n",
      "\n",
      "\n",
      "2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08  2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n",
      "\n",
      "# Evaluation Prompts\n",
      "\n",
      "Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.\n",
      "\n",
      "# Baselines\n",
      "\n",
      "We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).\n",
      "\n",
      "# Evaluation Setup\n",
      "\n",
      "We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@ evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top- value of 0.95 to generate  responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as\n",
      "\n",
      "pass@1 = 1 / k *  (pi), where pi denotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n",
      "\n",
      "1https://aider.chat\n",
      "\n",
      "2https://codeforces.com\n",
      "\n",
      "3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 12 ---\n",
      "\n",
      "# 3.1. DeepSeek-R1 Evaluation\n",
      "\n",
      "| Benchmark (Metric)              | Claude-3.5-             | GPT-4o | DeepSeek | OpenAI | OpenAI | DeepSeek |      |\n",
      "| ------------------------------- | ----------------------- | ------ | -------- | ------ | ------ | -------- | ---- |\n",
      "| Architecture                    | -                       | -      | MoE      | -      | -      | MoE      |      |\n",
      "| # Activated Params              | -                       | -      | 37B      | -      | -      | 37B      |      |\n",
      "| # Total Params                  | -                       | -      | 671B     | -      | -      | 671B     |      |\n",
      "| MMLU (Pass\\@1)                  | 88.3                    | 87.2   | 88.5     | 85.2   | 91.8   | 90.8     |      |\n",
      "| MMLU-Redux (EM)                 | 88.9                    | 88.0   | 89.1     | 86.7   | -      | 92.9     |      |\n",
      "| MMLU-Pro (EM)                   | 78.0                    | 72.6   | 75.9     | 80.3   | -      | 84.0     |      |\n",
      "| DROP (3-shot F1)                | 88.3                    | 83.7   | 91.6     | 83.9   | 90.2   | 92.2     |      |\n",
      "| English IF-Eval (Prompt Strict) | 86.5                    | 84.3   | 86.1     | 84.8   | -      | 83.3     |      |\n",
      "| GPQA Diamond (Pass\\@1)          | 65.0                    | 49.9   | 59.1     | 60.0   | 75.7   | 71.5     |      |\n",
      "| SimpleQA (Correct)              | 28.4                    | 38.2   | 24.9     | 7.0    | 47.0   | 30.1     |      |\n",
      "| FRAMES (Acc.)                   | 72.5                    | 80.5   | 73.3     | 76.9   | -      | 82.5     |      |\n",
      "| AlpacaEval2.0 (LC-winrate)      | 52.0                    | 51.1   | 70.0     | 57.8   | -      | 87.6     |      |\n",
      "| ArenaHard (GPT-4-1106)          | 85.2                    | 80.4   | 85.5     | 92.0   | -      | 92.3     |      |\n",
      "| LiveCodeBench (Pass\\@1-COT)     | 38.9                    | 32.9   | 36.2     | 53.8   | 63.4   | 65.9     |      |\n",
      "| Code                            | Codeforces (Percentile) | 20.3   | 23.6     | 58.7   | 93.4   | 96.6     | 96.3 |\n",
      "| Codeforces (Rating)             | 717                     | 759    | 1134     | 1820   | 2061   | 2029     |      |\n",
      "| SWE Verified (Resolved)         | 50.8                    | 38.8   | 42.0     | 41.6   | 48.9   | 49.2     |      |\n",
      "| Aider-Polyglot (Acc.)           | 45.3                    | 16.0   | 49.6     | 32.9   | 61.7   | 53.3     |      |\n",
      "| AIME 2024 (Pass\\@1)             | 16.0                    | 9.3    | 39.2     | 63.6   | 79.2   | 79.8     |      |\n",
      "| Math                            | MATH-500 (Pass\\@1)      | 78.3   | 74.6     | 90.2   | 90.0   | 96.4     | 97.3 |\n",
      "| CNMO 2024 (Pass\\@1)             | 13.1                    | 10.8   | 43.2     | 67.6   | -      | 78.8     |      |\n",
      "| CLUEWSC (EM)                    | 85.4                    | 87.9   | 90.9     | 89.9   | -      | 92.8     |      |\n",
      "| Chinese C-Eval (EM)             | 76.7                    | 76.0   | 86.5     | 68.9   | -      | 91.8     |      |\n",
      "| C-SimpleQA (Correct)            | 55.4                    | 58.7   | 68.0     | 40.3   | -      | 63.7     |      |\n",
      "\n",
      "Table 4 | Comparison between DeepSeek-R1 and other representative models.\n",
      "\n",
      "For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%.\n",
      "\n",
      "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\n",
      "\n",
      "\n",
      "--- Document 13 ---\n",
      "\n",
      "\n",
      "DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n",
      "\n",
      "On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n",
      "\n",
      "# 3.2. Distilled Model Evaluation\n",
      "\n",
      "| Model                         | AIME 2024 |      | MATH-500 | GPQA | LiveCode Diamond | CodeForces Bench |\n",
      "| ----------------------------- | --------- | ---- | -------- | ---- | ---------------- | ---------------- |\n",
      "| GPT-4o-0513                   | 9.3       | 13.4 | 74.6     | 49.9 | 32.9             | 759              |\n",
      "| Claude-3.5-Sonnet-1022        | 16.0      | 26.7 | 78.3     | 65.0 | 38.9             | 717              |\n",
      "| OpenAI-o1-mini                | 63.6      | 80.0 | 90.0     | 60.0 | 53.8             | 1820             |\n",
      "| QwQ-32B-Preview               | 50.0      | 60.0 | 90.6     | 54.5 | 41.9             | 1316             |\n",
      "| DeepSeek-R1-Distill-Qwen-1.5B | 28.9      | 52.7 | 83.9     | 33.8 | 16.9             | 954              |\n",
      "| DeepSeek-R1-Distill-Qwen-7B   | 55.5      | 83.3 | 92.8     | 49.1 | 37.6             | 1189             |\n",
      "| DeepSeek-R1-Distill-Qwen-14B  | 69.7      | 80.0 | 93.9     | 59.1 | 53.1             | 1481             |\n",
      "| DeepSeek-R1-Distill-Qwen-32B  | 72.6      | 83.3 | 94.3     | 62.1 | 57.2             | 1691             |\n",
      "| DeepSeek-R1-Distill-Llama-8B  | 50.4      | 80.0 | 89.1     | 49.0 | 39.6             | 1205             |\n",
      "| DeepSeek-R1-Distill-Llama-70B | 70.0      | 86.7 | 94.5     | 65.2 | 57.5             | 1633             |\n",
      "\n",
      "Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.\n",
      "\n",
      "As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n",
      "\n",
      "# 4. Discussion\n",
      "\n",
      "# 4.1. Distillation v.s. Reinforcement Learning\n",
      "\n",
      "In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n",
      "\n",
      "To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 14 ---\n",
      "\n",
      "AIME 2024\n",
      "# Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\n",
      "\n",
      "| Model                        | pass\\@1 | cons\\@64 | pass\\@1 | pass\\@1 | pass\\@1 |\n",
      "| ---------------------------- | ------- | -------- | ------- | ------- | ------- |\n",
      "| QwQ-32B-Preview              | 50.0    | 60.0     | 90.6    | 54.5    | 41.9    |\n",
      "| DeepSeek-R1-Zero-Qwen-32B    | 47.0    | 60.0     | 91.6    | 55.0    | 40.2    |\n",
      "| DeepSeek-R1-Distill-Qwen-32B | 72.6    | 83.3     | 94.3    | 62.1    | 57.2    |\n",
      "\n",
      "RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning.\n",
      "\n",
      "# 4.2. Unsuccessful Attempts\n",
      "\n",
      "In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n",
      "\n",
      "# Process Reward Model (PRM)\n",
      "\n",
      "PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n",
      "\n",
      "# Monte Carlo Tree Search (MCTS)\n",
      "\n",
      "Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Document 15 ---\n",
      "\n",
      "\n",
      "exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGos core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.\n",
      "\n",
      "In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge.\n",
      "\n",
      "# 5. Conclusion, Limitations, and Future Work\n",
      "\n",
      "In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n",
      "\n",
      "We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n",
      "\n",
      "In the future, we plan to invest in research across the following directions for DeepSeek-R1.\n",
      "\n",
      "- General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      "- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n",
      "- Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n",
      "- Software Engineering Tasks: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(22, len(documents))):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(documents[i].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91d2c6",
   "metadata": {},
   "source": [
    "### Embedding Models\n",
    "\n",
    "| MODEL                  | PAGES PER DOLLAR | PERFORMANCE ON MTEB EVAL | MAX INPUT |\n",
    "|------------------------|------------------|---------------------------|-----------|\n",
    "| text-embedding-3-small | 62,500           | 62.3%                     | 8191      |\n",
    "| text-embedding-3-large | 9,615            | 64.6%                     | 8191      |\n",
    "| text-embedding-ada-002 | 12,500           | 61.0%                     | 8191      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a52c370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Split\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Embeddings \n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# 4. Vector DB: FAISS\n",
    "db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# 5. Retrieval\n",
    "# 5-1. Ensemble Retrieval\n",
    "\n",
    "# bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    split_docs,\n",
    ")\n",
    "bm25_retriever.k = 10  # BM25Retriever\n",
    "\n",
    "# FAISS retriever\n",
    "embedding = OpenAIEmbeddings()  \n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    split_docs,\n",
    "    embedding,\n",
    ")\n",
    "\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.2, 0.8],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9765fd",
   "metadata": {},
   "source": [
    "### Reranker Model\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"images/reranker-benchmark.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a2282ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-2. Rerank Model\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "compressor = FlashrankRerank(\n",
    "    model=\"ms-marco-MultiBERT-L-12\",\n",
    "    top_n=10)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f46ad4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is DeepSeek-R1-Zero?\"\n",
    "# compression_retriever.invoke(query)[0].page_content\n",
    "# compression_retriever.invoke(query)[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7290a96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compression Retriever]\n",
      "Content: However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain\n",
      "\n",
      "Content: DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities\n",
      "\n",
      "Content: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning\n",
      "\n",
      "Content: During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.\n",
      "\n",
      "Content: which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "\n",
      "Content: # Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
      "\n",
      "Content: - General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      "- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n",
      "\n",
      "Content: - Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n",
      "\n",
      "Content: of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n",
      "\n",
      "Content: - Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "\n",
      "[Ensemble Retriever]\n",
      "Content: DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities\n",
      "\n",
      "Content: - Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "\n",
      "Content: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning\n",
      "\n",
      "Content: which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "\n",
      "Content: However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain\n",
      "\n",
      "Content: # 3.1. DeepSeek-R1 Evaluation\n",
      "\n",
      "Content: # Figure 1\n",
      "\n",
      "Benchmark performance of DeepSeek-R1.\n",
      "\n",
      "Content: During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.\n",
      "\n",
      "Content: of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n",
      "\n",
      "Content: Table 4 | Comparison between DeepSeek-R1 and other representative models.\n",
      "\n",
      "Content: PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs\n",
      "\n",
      "Content: # Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
      "\n",
      "Content: - Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
      "- Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between  tags.\n",
      "\n",
      "Content: - General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      "- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n",
      "\n",
      "Content: creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768\n",
      "\n",
      "Content: - Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n",
      "\n",
      "Content: we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an\n",
      "\n",
      "Content: # Baselines\n",
      "\n",
      "We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).\n",
      "\n",
      "# Evaluation Setup\n",
      "\n",
      "[FAISS Retriever]\n",
      "Content: We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning\n",
      "\n",
      "Content: DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities\n",
      "\n",
      "Content: which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "\n",
      "Content: However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain\n",
      "\n",
      "Content: # 3.1. DeepSeek-R1 Evaluation\n",
      "\n",
      "Content: # Figure 1\n",
      "\n",
      "Benchmark performance of DeepSeek-R1.\n",
      "\n",
      "Content: During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.\n",
      "\n",
      "Content: - Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "\n",
      "Content: of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n",
      "\n",
      "Content: Table 4 | Comparison between DeepSeek-R1 and other representative models.\n",
      "\n",
      "[BM25 Retriever]\n",
      "Content: PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs\n",
      "\n",
      "Content: - Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "\n",
      "Content: # Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
      "\n",
      "Content: DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities\n",
      "\n",
      "Content: - Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
      "- Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between  tags.\n",
      "\n",
      "Content: - General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      "- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n",
      "\n",
      "Content: creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768\n",
      "\n",
      "Content: - Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n",
      "\n",
      "Content: we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an\n",
      "\n",
      "Content: # Baselines\n",
      "\n",
      "We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).\n",
      "\n",
      "# Evaluation Setup\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is DeepSeek-R1-Zero?\"\n",
    "\n",
    "compression_retriever_result = compression_retriever.invoke(query)\n",
    "ensemble_result = ensemble_retriever.invoke(query)\n",
    "bm25_result = bm25_retriever.invoke(query)\n",
    "faiss_result = faiss_retriever.invoke(query)\n",
    "\n",
    "print(\"[compression Retriever]\")\n",
    "for doc in compression_retriever_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[Ensemble Retriever]\")\n",
    "for doc in ensemble_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[FAISS Retriever]\")\n",
    "for doc in faiss_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[BM25 Retriever]\")\n",
    "for doc in bm25_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0dda19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_retriever = compression_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b73c6",
   "metadata": {},
   "source": [
    "# PDF LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d2fcdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 5. Create Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \n",
    "Your primary mission is to answer questions based on provided context or chat history.\n",
    "Ensure your response is concise and directly addresses the question without any additional narration.\n",
    "\n",
    "###\n",
    "\n",
    "You may consider the previous conversation history to answer the question.\n",
    "\n",
    "# Here's the previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "###\n",
    "\n",
    "Your final answer should be written concisely (but include important numerical values, technical terms, jargon, and names), followed by the source of the information.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. Carefully read and understand the context provided.\n",
    "2. Identify the key information related to the question within the context.\n",
    "3. Formulate a concise answer based on the relevant information.\n",
    "4. Ensure your final answer directly addresses the question.\n",
    "5. List the source of the answer in bullet points, which must be a file name (with a page number) or URL from the context. Omit if the answer is based on previous conversation or if the source cannot be found.\n",
    "\n",
    "# Output Format:\n",
    "[Your final answer here, with numerical values, technical terms, jargon, and names in their original language]\n",
    "\n",
    "**Source**(Optional)\n",
    "- (Source of the answer, must be a file name(with a page number) or URL from the context. Omit if the answer is based on previous conversation or can't find the source.)\n",
    "- (list more if there are multiple sources)\n",
    "- ...\n",
    "\n",
    "###\n",
    "\n",
    "Remember:\n",
    "- It's crucial to base your answer solely on the **provided context** or **chat history**. \n",
    "- DO NOT use any external knowledge or information not present in the given materials.\n",
    "- If a user asks based on the previous conversation, but if there's no previous conversation or not enough information, you should answer that you don't know.\n",
    "\n",
    "###\n",
    "\n",
    "# Here is the user's question:\n",
    "{question}\n",
    "\n",
    "# Here is the context that you should use to answer the question:\n",
    "{context}\n",
    "\n",
    "# Your final answer to the user's question:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f784bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 7. LLM Generator \n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# 8. LLM Chain\n",
    "chain = (\n",
    "    # {\n",
    "    #     \"question\": RunnablePassthrough(),\n",
    "    #     \"context\": pdf_retriever, \n",
    "    #     \"chat_history\": lambda _: [],\n",
    "    #     }\n",
    "\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"context\": itemgetter(\"context\"), \n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "pdf_chain = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9fff6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is DeepSeek-R1-Zero?\"\n",
    "# response = pdf_chain.invoke(question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe416b35",
   "metadata": {},
   "source": [
    "# 1. State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "224d630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "\n",
    "    question: Annotated[List[str], add_messages]\n",
    "    context: Annotated[str, \"Context\"] \n",
    "    answer: Annotated[str, \"Answer\"]  \n",
    "    messages: Annotated[list, add_messages]    \n",
    "    \n",
    "    relevance: Annotated[str, \"Relevance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3c4d9",
   "metadata": {},
   "source": [
    "# 2. Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e8a1658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.messages import messages_to_history\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from tools.utils import format_docs\n",
    "\n",
    "\n",
    "# Node 1. Retrieve Node\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    latest_question = state[\"question\"][-1].content\n",
    "\n",
    "    # retrieved_docs = pdf_retriever.invoke(latest_question)\n",
    "    retrieved_docs = pdf_retriever.invoke(latest_question)\n",
    "\n",
    "    retrieved_docs = format_docs(retrieved_docs)\n",
    "\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "# Node 2. Answer Node\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    latest_question = state[\"question\"][-1].content\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    response = pdf_chain.invoke(    \n",
    "        {\n",
    "            \"question\": latest_question,\n",
    "            \"context\": context,\n",
    "            \"chat_history\": messages_to_history(state[\"messages\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"messages\": [(\"user\", latest_question), (\"assistant\", response)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d5029942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Query Rewrite Prompt\n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"Reformulate the given question to enhance its effectiveness for vectorstore retrieval.\n",
    "\n",
    "- Analyze the initial question to identify areas for improvement such as specificity, clarity, and relevance.\n",
    "- Consider the context and potential keywords that would optimize retrieval.\n",
    "- Maintain the intent of the original question while enhancing its structure and vocabulary.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Understand the Original Question**: Identify the core intent and any keywords.\n",
    "2. **Enhance Clarity**: Simplify language and ensure the question is direct and to the point.\n",
    "3. **Optimize for Retrieval**: Add or rearrange keywords for better alignment with vectorstore indexing.\n",
    "4. **Review**: Ensure the improved question accurately reflects the original intent and is free of ambiguity.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "- Provide a single, improved question.\n",
    "- Do not include any introductory or explanatory text; only the reformulated question.\n",
    "\n",
    "# Examples\n",
    "\n",
    "**Input**: \n",
    "\"What are the benefits of using renewable energy sources over fossil fuels?\"\n",
    "\n",
    "**Output**: \n",
    "\"How do renewable energy sources compare to fossil fuels in terms of benefits?\"\n",
    "\n",
    "**Input**: \n",
    "\"How does climate change impact polar bear populations?\"\n",
    "\n",
    "**Output**: \n",
    "\"What effects does climate change have on polar bear populations?\"\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Ensure the improved question is concise and contextually relevant.\n",
    "- Avoid altering the fundamental intent or meaning of the original question.\n",
    "\n",
    "\n",
    "[REMEMBER] Re-written question should be in the same language as the original question.\n",
    "\n",
    "# Here is the original question that needs to be rewritten:\n",
    "{question}\n",
    "\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = (\n",
    "    re_write_prompt \n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4da52d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is DeepSeek-R1-Zero?\"\n",
    "\n",
    "# question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "81d8f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3. Query Rewrite Node\n",
    "# def query_rewrite(state: GraphState) -> GraphState:\n",
    "#     latest_question = state[\"question\"][-1].content\n",
    "#     question_rewritten = question_rewriter.invoke({\"question\": latest_question})\n",
    "#     return {\"question\": question_rewritten}\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def query_rewrite(state: GraphState) -> GraphState:\n",
    "    latest_question = state[\"question\"][-1].content\n",
    "    question_rewritten = question_rewriter.invoke({\"question\": latest_question})\n",
    "    \n",
    "    return {\"question\": [HumanMessage(content=question_rewritten)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1d1a10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.evaluator import GroundednessChecker\n",
    "\n",
    "# Node 4. Relevance Check Node\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "\n",
    "    question_answer_relevant = GroundednessChecker(\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"question-retrieval\"\n",
    "    ).create()\n",
    "\n",
    "    response = question_answer_relevant.invoke(\n",
    "        {\"question\": state[\"question\"][-1].content, \"context\": state[\"context\"]}\n",
    "    )\n",
    "\n",
    "    return {\"relevance\": response.score}\n",
    "\n",
    "\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "    if state[\"relevance\"] == \"yes\":\n",
    "        return \"relevant\"\n",
    "    else:\n",
    "        return \"not relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7395580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 5. Web Search Node \n",
    "def web_search(state: GraphState) -> GraphState:\n",
    "    tavily_tool = TavilySearch()\n",
    "\n",
    "    latest_question = state[\"question\"][-1].content\n",
    "\n",
    "    search_result = tavily_tool.search(\n",
    "        query=latest_question,  \n",
    "        topic=\"general\",\n",
    "        max_results=5,\n",
    "        format_output=True,\n",
    "    )\n",
    "\n",
    "    return {\"context\": search_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc925e4",
   "metadata": {},
   "source": [
    "# 3. Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c2980c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Query Rewrite Node\n",
    "workflow.add_node(\"query_rewrite\", query_rewrite)\n",
    "workflow.add_node(\"relevance_check\", relevance_check)\n",
    "workflow.add_node(\"retrieve\", retrieve_document)\n",
    "workflow.add_node(\"llm_answer\", llm_answer)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "workflow.add_edge(\"query_rewrite\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\") \n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\", \n",
    "    is_relevant,\n",
    "    {\n",
    "        \"relevant\": \"llm_answer\",  \n",
    "        \"not relevant\": \"web_search\", \n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"llm_answer\") \n",
    "workflow.add_edge(\"llm_answer\", END)  \n",
    "\n",
    "workflow.set_entry_point(\"query_rewrite\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438e4c5",
   "metadata": {},
   "source": [
    "# 4. Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53143f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e85941",
   "metadata": {},
   "source": [
    "# 5. Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea53907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the DeepSeek-R1-Zero technology?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content># Self-evolution Process of DeepSeek-R1-Zero\n",
      "\n",
      "The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n",
      "\n",
      "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement.</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content># Drawback of DeepSeek-R1-Zero\n",
      "\n",
      "Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\n",
      "\n",
      "# 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start</content><source>data/Deepseek-r1.pdf</source><page>9</page></document>\n",
      "<document><content># Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n",
      "\n",
      "DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.</content><source>data/Deepseek-r1.pdf</source><page>1</page></document>\n",
      "<document><content>We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and</content><source>data/Deepseek-r1.pdf</source><page>1</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "DeepSeek-R1-Zero is a first-generation reasoning model trained solely via large-scale reinforcement learning (RL) without any supervised fine-tuning (SFT). It autonomously develops strong and sophisticated reasoning behaviors, significantly improving its performance on reasoning benchmarks like AIME 2024, where its pass@1 score rises from 15.6% to 71.0%, and further to 86.7% with majority voting, matching or exceeding models like OpenAI-o1-0912. The model naturally learns to allocate more thinking time and generate extensive reasoning tokens (hundreds to thousands) to solve complex tasks. However, it faces drawbacks such as poor readability and language mixing, which led to the development of DeepSeek-R1 with cold-start data and multi-stage training to address these issues.\n",
      "\n",
      "**Source**  \n",
      "- data/Deepseek-r1.pdf (pages 1, 3, 6, 7, 8, 9)\n",
      "('user', 'What is the DeepSeek-R1-Zero technology?')\n",
      "('assistant', 'DeepSeek-R1-Zero is a first-generation reasoning model trained solely via large-scale reinforcement learning (RL) without any supervised fine-tuning (SFT). It autonomously develops strong and sophisticated reasoning behaviors, significantly improving its performance on reasoning benchmarks like AIME 2024, where its pass@1 score rises from 15.6% to 71.0%, and further to 86.7% with majority voting, matching or exceeding models like OpenAI-o1-0912. The model naturally learns to allocate more thinking time and generate extensive reasoning tokens (hundreds to thousands) to solve complex tasks. However, it faces drawbacks such as poor readability and language mixing, which led to the development of DeepSeek-R1 with cold-start data and multi-stage training to address these issues.\\n\\n**Source**  \\n- data/Deepseek-r1.pdf (pages 1, 3, 6, 7, 8, 9)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import invoke_graph, stream_graph, random_uuid\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# Chunk 600 overlap 100\n",
    "inputs = GraphState(question=[HumanMessage(content=\"What is DeepSeek-R1-Zero?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the DeepSeek-R1-Zero technology?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content># Table 3\n",
      "\n",
      "| An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.\n",
      "\n",
      "# Drawback of DeepSeek-R1-Zero</content><source>data/Deepseek-r1.pdf</source><page>9</page></document>\n",
      "<document><content>DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL,</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "DeepSeek-R1-Zero is a reinforcement learning (RL) based model designed to enhance reasoning capabilities of large language models (LLMs) without relying on cold-start data. It starts RL training directly from a base model using a simple template that guides it to produce a reasoning process followed by a final answer. Throughout training, it autonomously improves reasoning skills such as self-verification, reflection, and generating long chains of thought (CoTs). DeepSeek-R1-Zero shows significant performance gains on reasoning benchmarks like AIME 2024, improving pass@1 scores from 15.6% to 71.0%, and further to 86.7% with majority voting, surpassing some OpenAI models. It also exhibits emergent behaviors like an \"aha moment\" where it learns to rethink problems more deeply. However, it faces challenges like poor readability and language mixing, which are addressed in the subsequent DeepSeek-R1 model.\n",
      "\n",
      "**Source**  \n",
      "- data/Deepseek-r1.pdf (pages 3-9, 16)\n",
      "('user', 'What is the DeepSeek-R1-Zero technology?')\n",
      "('assistant', 'DeepSeek-R1-Zero is a reinforcement learning (RL) based model designed to enhance reasoning capabilities of large language models (LLMs) without relying on cold-start data. It starts RL training directly from a base model using a simple template that guides it to produce a reasoning process followed by a final answer. Throughout training, it autonomously improves reasoning skills such as self-verification, reflection, and generating long chains of thought (CoTs). DeepSeek-R1-Zero shows significant performance gains on reasoning benchmarks like AIME 2024, improving pass@1 scores from 15.6% to 71.0%, and further to 86.7% with majority voting, surpassing some OpenAI models. It also exhibits emergent behaviors like an \"aha moment\" where it learns to rethink problems more deeply. However, it faces challenges like poor readability and language mixing, which are addressed in the subsequent DeepSeek-R1 model.\\n\\n**Source**  \\n- data/Deepseek-r1.pdf (pages 3-9, 16)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# Chunk 300 overlap 50\n",
    "inputs = GraphState(question=[HumanMessage(content=\"What is DeepSeek-R1-Zero?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1dbca61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the CeADAR initiative in Ireland?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.</content><source>data/Deepseek-r1.pdf</source><page>10</page></document>\n",
      "<document><content>- Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
      "- Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between  tags.</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>- We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in</content><source>data/Deepseek-r1.pdf</source><page>10</page></document>\n",
      "<document><content># Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content>This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>- General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      "- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "no\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mweb_search\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<document><title>CeADAR Ireland</title><url>https://th.linkedin.com/company/ceadar-ireland?trk=ppro_cprof</url><content>CeADAR is Ireland's National Centre for AI. Funded by EI and the IDA, CeADAR has more than 90 member companies across a wide span of industries and is one</content></document>\n",
      "<document><title>CeADAR</title><url>https://www.enterprise-ireland.com/en/supports/become-more-innovative/technology-centres/ceadar</url><content>CeADAR is the National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI in Ireland. CeADAR has more than 90 member companies ranging from multinationals to indigenous SMEs spanning every industry vertical. The primary work of the Centre is on cutting-edge applied AI innovation and developing and deploying industry prototypes and market-ready solutions to companies. We support companies in finding funding and investment, ecosystem networking in Ireland and Europe, training with and access to our powerful in-house supercomputer. CeADAR also helps its member companies benchmark their state of AI maturity and plan their AI strategy. CeADAR is very active in European funded projects, and in producing a succession of spinout companies. CeADAR is one of only 9 Gold i-spaces in the EU and has won 2 national awards for contributions to the development and implementation of AI strategy to businesses in Ireland. * Driving accelerated AI adoption in Irish enterprise. * Fostering a strong national AI innovation ecosystem.</content><raw>How can we help?\n",
      "\n",
      "* [Im an Irish company](/en/)\n",
      "* [I want to connect with Irish companies](/en/global)\n",
      "\n",
      "* [Search](javascript:void(0))\n",
      "* [Submit a Claim](/en/supports/claims)\n",
      "* [Contact Us](/en/contact-us)\n",
      "\n",
      "* [Supports](javascript:void(0)) \n",
      "\n",
      "  [Back](javascript:void(0))\n",
      "\n",
      "  Supports\n",
      "\n",
      "  By Support\n",
      "\n",
      "  + [Market access](/en/supports/market-access)\n",
      "  + [Capability development](/en/supports/capability-development)\n",
      "  + [Funding and grants](/en/supports/funding-and-grants)\n",
      "\n",
      "  By Business\n",
      "\n",
      "  + [Start-ups](/en/supports/start-ups)\n",
      "  + [SMEs](/en/supports/for-smes)\n",
      "  + [Large companies](/en/supports/for-large-companies)\n",
      "  + [Researchers](/en/supports/for-research-organisations)\n",
      "\n",
      "  By Goal\n",
      "\n",
      "  + [Expand your business](/en/supports/expand-your-business)\n",
      "  + [Grow internationally](/en/supports/grow-internationally)\n",
      "  + [Become more sustainable](/en/sustainability)\n",
      "  + [Become more innovative](/en/supports/become-more-innovative)\n",
      "  + [Become more competitive](/en/supports/become-more-competitive)\n",
      "  + [Enhance sales and marketing](/en/supports/enhance-sales-and-marketing)\n",
      "* [About](javascript:void(0)) \n",
      "\n",
      "  [Back](javascript:void(0))\n",
      "\n",
      "  About\n",
      "\n",
      "  Who we are\n",
      "\n",
      "  + [About Enterprise Ireland](/en/about-enterprise-ireland)\n",
      "  + [Our team](/en/about-enterprise-ireland/our-team)\n",
      "  + [Careers at Enterprise Ireland](/en/careers-at-enterprise-ireland)\n",
      "\n",
      "  What we do\n",
      "\n",
      "  + [How we help businesses grow](/en/our-services)\n",
      "  + [Success stories](/en/success-stories)\n",
      "  + [Sectors we work in](/en/sectors)\n",
      "  + [Regional development](/en/supports/regional-development)\n",
      "* [News and Events](javascript:void(0)) \n",
      "\n",
      "  [Back](javascript:void(0))\n",
      "\n",
      "  News and Events\n",
      "\n",
      "  News\n",
      "\n",
      "  + [News](/en/news)\n",
      "  + [Publications and reports](/en/publications)\n",
      "  + [Press room](/en/press-room)\n",
      "\n",
      "  Events\n",
      "\n",
      "  + [Upcoming events](/en/events)\n",
      "\n",
      "  Insights\n",
      "\n",
      "  + [Insights and webinars](/en/insights-webinars)\n",
      "  + [Market Research Center](/en/market-research-centre)\n",
      "\n",
      "* [Submit a Claim](/en/supports/claims)\n",
      "* [Contact Us](/en/contact-us)\n",
      "\n",
      "- [Search](javascript:void(0))\n",
      "\n",
      "# CeADAR\n",
      "\n",
      "## What is CeADAR\n",
      "\n",
      "CeADAR is the National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI in Ireland. CeADAR has more than 90 member companies ranging from multinationals to indigenous SMEs spanning every industry vertical. The primary work of the Centre is on cutting-edge applied AI innovation and developing and deploying industry prototypes and market-ready solutions to companies.\n",
      "\n",
      "We support companies in finding funding and investment, ecosystem networking in Ireland and Europe, training with and access to our powerful in-house supercomputer. CeADAR also helps its member companies benchmark their state of AI maturity and plan their AI strategy. CeADAR is very active in European funded projects, and in producing a succession of spinout companies.\n",
      "\n",
      "The Centre is the focal point of a thriving national ecosystem delivering frequent seminars, conferences, and members networking events throughout the year. CeADAR is one of only 9 Gold i-spaces in the EU and has won 2 national awards for contributions to the development and implementation of AI strategy to businesses in Ireland. The Centre contributes on various task forces in the OECD, the Department of Enterprise, national standards bodies, and business representative groups.\n",
      "\n",
      "**Expertise:**\n",
      "\n",
      "* Driving accelerated AI adoption in Irish enterprise\n",
      "* Creating and delivering value for clients\n",
      "* Strengthening client AI capability\n",
      "* Building sustained competitive advantage for clients\n",
      "* Supporting clients in finding investment\n",
      "* Fostering a strong national AI innovation ecosystem\n",
      "* Provision of AI training, upskilling and talent\n",
      "* Promotion of good AI governance and trustworthy AI\n",
      "\n",
      "**Research performed by:**\n",
      "\n",
      "* CeADAR in University College Dublin\n",
      "\n",
      "## Contact\n",
      "\n",
      "#### John Lonsdale\n",
      "\n",
      "Chief Executive Officer\n",
      "\n",
      "#### John Lonsdale\n",
      "\n",
      "Chief Executive Officer\n",
      "\n",
      "**Phone:** [+353 1 716 5716](tel:+35317165716)\n",
      "\n",
      "**Web:** [www.ceadar.ie](https://ceadar.ie/)\n",
      "\n",
      "**Twitter:** [@CeADARIreland](https://twitter.com/ceadarireland)\n",
      "\n",
      "[john.lonsdale@ucd.ie](mailto:john.lonsdale@ucd.ie)\n",
      "\n",
      " 2025 Enterprise Ireland All rights reserved\n",
      "\n",
      "VAT No: IE9590828H</raw></document>\n",
      "<document><title>CeADAR-Centre for Applied Data Analytics Research</title><url>https://stip.oecd.org/stip/interactive-dashboards/policy-initiatives/2025%2Fdata%2FpolicyInitiatives%2F200002846</url><content>The Centre for Applied Data Analytics Research (CeADAR) is Ireland's national centre for AI. A technology centre established to support</content></document>\n",
      "<document><title>CeADAR: Ireland's Centre for AI and Applied Data Analytics</title><url>https://www.clustercollaboration.eu/content/ceadar-irelands-centre-ai-and-applied-data-analytics</url><content># CeADAR: Ireland's Centre for AI and Applied Data Analytics CeADAR: Ireland's Centre for AI and Applied Data Analytics. The Centre for AI & Applied Data Analytics (CeADAR) is the national market-led technology centre for the research, development and deployment of AI & big data analytics technology and innovation, focusing on developing tools, techniques and technologies that enable people, organisations and industries to use AI & analytics for better decision making and competitive advantage. The aim of CeADAR is to rapidly prototype and deliver AI & analytics technology and solutions to industry from an agenda that is solely defined by the needs of the market. The Centres primary outputs are prototypes, demonstrators and bespoke solutions co-developed with individual industry members. The Centre has an extensive catalogue of technology demonstrators (>60), IP and AI/Machine Learning/Analytics technology reviews which are immediately available to members for evaluation.</content><raw>## Warning message\n",
      "\n",
      "Could not find appropriate formatter field to render *Primary Contact Telephone*.\n",
      "\n",
      "# CeADAR: Ireland's Centre for AI and Applied Data Analytics CeADAR: Ireland's Centre for AI and Applied Data Analytics\n",
      "\n",
      "Ireland: Eastern and Midland (Ireland)\n",
      "\n",
      "## Overview\n",
      "\n",
      "The Centre for AI & Applied Data Analytics (CeADAR) is the national market-led technology centre for the research, development and deployment of AI & big data analytics technology and innovation, focusing on developing tools, techniques and technologies that enable people, organisations and industries to use AI & analytics for better decision making and competitive advantage.\n",
      "\n",
      "The aim of CeADAR is to rapidly prototype and deliver AI & analytics technology and solutions to industry from an agenda that is solely defined by the needs of the market. The Centres primary outputs are prototypes, demonstrators and bespoke solutions co-developed with individual industry members.\n",
      "\n",
      "The Centre is also one the the EU's 30 Digital Innovation Hubs in AI, an accredited i-Space from the BDVA.\n",
      "\n",
      "Along with NextGeneration in Dublin,the Centre set up the European Data Science and AI Awards (The DatScis) which showcases and celebrates the talent and success of individuals and companies in AI across the EU  \n",
      " The CeADAR centre [www.ceadar.ie](http://www.ceadar.ie) has a high bandwidth, high speed data centre to enable the real-time processing of very large quantities of fast streaming data of the sort that is experienced in large industrial process control and prediction applications.CeADAR maintains its own data centre as well as using web services from various providers. Recently the Centre was awarded national funding to acquire a supercomputer which will also be accessible to the nation ecosystem of SMEs and MNCs  \n",
      " . The CeADAR Centre undertakes consultancy, contract research, participation in EU Horizon 2020 consortia, and develops up to 12 demonstrator projects per year in 2 cycles of 6 months each.\n",
      "\n",
      "Each project is proposed by the industry members and is focused onclose-to-market challenges.\n",
      "\n",
      "The Centre has an extensive catalogue of technology demonstrators (>60), IP and AI/Machine Learning/Analytics technology reviews which are immediately available to members for evaluation.  \n",
      " . Yes. service.\n",
      "\n",
      "##### Sectors\n",
      "\n",
      "Aeronautics & space - Agriculture - Agro-food - Construction & building sector - Consumer goods/products - Energy - Environment - Healthcare - Machinery - Manufacturing - Process Manufacturing - Professional Services - Retail & Wholesale - Transport & Logistics - Utilities and Oil & Gas\n",
      "\n",
      "##### Technology\n",
      "\n",
      "Monitoring and control - Smart Manufacturing / Industry 4.0 - Artificial intelligence - Natural Language Processing - Signal sensing - Video / image processing - Data analytics (predictive analysis, in-memory analytics, network analysis, advanced cluster analysis) - Data management (acquisition, extraction and integration) - Machine learning - Text & data mining - Block chain - IoT - Edge computing - Security and connectivity\n",
      "\n",
      "[Visit our website](http://www.ceadar.ie)\n",
      "\n",
      " </raw></document>\n",
      "<document><title>CeADAR and Equal1 Partner to Advance Ireland's ...</title><url>https://quantumcomputingreport.com/ceadar-and-equal1-partner-to-advance-irelands-quantum-ai-ecosystem/</url><content># CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem. CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem. Equal1, developer of Irelands first quantum computer *Bell-1*, has signed a Memorandum of Understanding (MoU) with CeADAR, Irelands Centre for Applied AI, to accelerate the national Quantum-AI infrastructure. The partnership targets joint development of funded R&D proposals and aims to build a framework supporting Irish business and academic adoption of hybrid quantum-AI systems. The collaboration will focus on integrating Equal1s compact quantum servers with AI workflows, leveraging CeADARs enterprise outreach to promote adoption. Equal1s approach, termed Quantum Computing 2.0, features a monolithic design where quantum processors and control electronics are integrated on a single chip, eliminating the need for bulky cryogenic systems. By combining technical innovation with ecosystem building, the CeADAREqual1 collaboration seeks to position Ireland as a leader in quantum-AI convergence, emphasizing strategic competitiveness in emerging technologies across Europe. The initiative will support national sovereignty in AI and quantum, foster talent development, and enable broader access to quantum-enhanced tools for industry.</content><raw>[Skip to content](#content)\n",
      "\n",
      "# CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem\n",
      "\n",
      "CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem\n",
      "\n",
      "[Equal1](https://www.equal1.com/), developer of Irelands first quantum computer *[Bell-1](https://www.equal1.com/bell-1)*, has signed a Memorandum of Understanding (MoU) with [CeADAR](https://ceadar.ie/), Irelands Centre for Applied AI, to accelerate the national Quantum-AI infrastructure. The partnership targets joint development of funded R&D proposals and aims to build a framework supporting Irish business and academic adoption of hybrid quantum-AI systems. Bell-1 uses a silicon-based architecture designed for scalability and cost-efficiency, signaling a transition from research prototypes to deployable systems.\n",
      "\n",
      "The collaboration will focus on integrating Equal1s compact quantum servers with AI workflows, leveraging CeADARs enterprise outreach to promote adoption. Equal1s approach, termed Quantum Computing 2.0, features a monolithic design where quantum processors and control electronics are integrated on a single chip, eliminating the need for bulky cryogenic systems. This architecture supports a smaller form factor and improved system stability, aligning with practical deployment needs for data-intensive environments.\n",
      "\n",
      "By combining technical innovation with ecosystem building, the CeADAREqual1 collaboration seeks to position Ireland as a leader in quantum-AI convergence, emphasizing strategic competitiveness in emerging technologies across Europe. The initiative will support national sovereignty in AI and quantum, foster talent development, and enable broader access to quantum-enhanced tools for industry.\n",
      "\n",
      "Full announcement available [here](https://www.equal1.com/post/ceadar-and-equal1-mou).\n",
      "\n",
      "May 9, 2025\n",
      "\n",
      "[Mohamed Abdel-Kareem](https://quantumcomputingreport.com/author/mohamed-abdel-kareem/ \"Posts by Mohamed Abdel-Kareem\")2025-05-09T20:28:05-07:00\n",
      "\n",
      "[Page load link](#)\n",
      "\n",
      "[Go to Top](#)\n",
      "\n",
      " </raw></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "CeADAR is Ireland's National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI. It supports over 90 member companies across various industries by driving applied AI innovation, developing and deploying industry prototypes and market-ready AI solutions. CeADAR helps companies find funding, provides ecosystem networking in Ireland and Europe, offers training and access to a powerful in-house supercomputer, and assists in benchmarking AI maturity and strategy planning. It is one of only 9 Gold i-spaces in the EU, active in European projects, producing spinouts, and fostering a strong national AI innovation ecosystem.\n",
      "\n",
      "**Source:**\n",
      "- https://www.enterprise-ireland.com/en/supports/become-more-innovative/technology-centres/ceadar (full content)  \n",
      "- https://www.clustercollaboration.eu/content/ceadar-irelands-centre-ai-and-applied-data-analytics\n",
      "('user', 'What is the CeADAR initiative in Ireland?')\n",
      "('assistant', \"CeADAR is Ireland's National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI. It supports over 90 member companies across various industries by driving applied AI innovation, developing and deploying industry prototypes and market-ready AI solutions. CeADAR helps companies find funding, provides ecosystem networking in Ireland and Europe, offers training and access to a powerful in-house supercomputer, and assists in benchmarking AI maturity and strategy planning. It is one of only 9 Gold i-spaces in the EU, active in European projects, producing spinouts, and fostering a strong national AI innovation ecosystem.\\n\\n**Source:**\\n- https://www.enterprise-ireland.com/en/supports/become-more-innovative/technology-centres/ceadar (full content)  \\n- https://www.clustercollaboration.eu/content/ceadar-irelands-centre-ai-and-applied-data-analytics\")\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Chunk 600 overlap 100\n",
    "inputs = GraphState(question=[HumanMessage(content=\"what is CeADAR, Ireland?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5946fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the CeADAR initiative in Ireland?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?</content><source>data/Deepseek-r1.pdf</source><page>14</page></document>\n",
      "<document><content>- Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>- Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>the query is in a language other than English or Chinese. We aim to address this limitation in future updates.</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in</content><source>data/Deepseek-r1.pdf</source><page>10</page></document>\n",
      "<document><content>A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we</content><source>data/Deepseek-r1.pdf</source><page>10</page></document>\n",
      "<document><content>is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "no\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mweb_search\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<document><title>CeADAR Ireland</title><url>https://th.linkedin.com/company/ceadar-ireland?trk=ppro_cprof</url><content>CeADAR is Ireland's National Centre for AI. Funded by EI and the IDA, CeADAR has more than 90 member companies across a wide span of industries and is one</content></document>\n",
      "<document><title>CeADAR</title><url>https://www.enterprise-ireland.com/en/supports/become-more-innovative/technology-centres/ceadar</url><content>CeADAR is the National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI in Ireland. CeADAR has more than 90 member companies ranging from multinationals to indigenous SMEs spanning every industry vertical. The primary work of the Centre is on cutting-edge applied AI innovation and developing and deploying industry prototypes and market-ready solutions to companies. We support companies in finding funding and investment, ecosystem networking in Ireland and Europe, training with and access to our powerful in-house supercomputer. CeADAR also helps its member companies benchmark their state of AI maturity and plan their AI strategy. CeADAR is very active in European funded projects, and in producing a succession of spinout companies. CeADAR is one of only 9 Gold i-spaces in the EU and has won 2 national awards for contributions to the development and implementation of AI strategy to businesses in Ireland. * Driving accelerated AI adoption in Irish enterprise. * Fostering a strong national AI innovation ecosystem.</content><raw>How can we help?\n",
      "\n",
      "* [Im an Irish company](/en/)\n",
      "* [I want to connect with Irish companies](/en/global)\n",
      "\n",
      "* [Search](javascript:void(0))\n",
      "* [Submit a Claim](/en/supports/claims)\n",
      "* [Contact Us](/en/contact-us)\n",
      "\n",
      "* [Supports](javascript:void(0)) \n",
      "\n",
      "  [Back](javascript:void(0))\n",
      "\n",
      "  Supports\n",
      "\n",
      "  By Support\n",
      "\n",
      "  + [Market access](/en/supports/market-access)\n",
      "  + [Capability development](/en/supports/capability-development)\n",
      "  + [Funding and grants](/en/supports/funding-and-grants)\n",
      "\n",
      "  By Business\n",
      "\n",
      "  + [Start-ups](/en/supports/start-ups)\n",
      "  + [SMEs](/en/supports/for-smes)\n",
      "  + [Large companies](/en/supports/for-large-companies)\n",
      "  + [Researchers](/en/supports/for-research-organisations)\n",
      "\n",
      "  By Goal\n",
      "\n",
      "  + [Expand your business](/en/supports/expand-your-business)\n",
      "  + [Grow internationally](/en/supports/grow-internationally)\n",
      "  + [Become more sustainable](/en/sustainability)\n",
      "  + [Become more innovative](/en/supports/become-more-innovative)\n",
      "  + [Become more competitive](/en/supports/become-more-competitive)\n",
      "  + [Enhance sales and marketing](/en/supports/enhance-sales-and-marketing)\n",
      "* [About](javascript:void(0)) \n",
      "\n",
      "  [Back](javascript:void(0))\n",
      "\n",
      "  About\n",
      "\n",
      "  Who we are\n",
      "\n",
      "  + [About Enterprise Ireland](/en/about-enterprise-ireland)\n",
      "  + [Our team](/en/about-enterprise-ireland/our-team)\n",
      "  + [Careers at Enterprise Ireland](/en/careers-at-enterprise-ireland)\n",
      "\n",
      "  What we do\n",
      "\n",
      "  + [How we help businesses grow](/en/our-services)\n",
      "  + [Success stories](/en/success-stories)\n",
      "  + [Sectors we work in](/en/sectors)\n",
      "  + [Regional development](/en/supports/regional-development)\n",
      "* [News and Events](javascript:void(0)) \n",
      "\n",
      "  [Back](javascript:void(0))\n",
      "\n",
      "  News and Events\n",
      "\n",
      "  News\n",
      "\n",
      "  + [News](/en/news)\n",
      "  + [Publications and reports](/en/publications)\n",
      "  + [Press room](/en/press-room)\n",
      "\n",
      "  Events\n",
      "\n",
      "  + [Upcoming events](/en/events)\n",
      "\n",
      "  Insights\n",
      "\n",
      "  + [Insights and webinars](/en/insights-webinars)\n",
      "  + [Market Research Center](/en/market-research-centre)\n",
      "\n",
      "* [Submit a Claim](/en/supports/claims)\n",
      "* [Contact Us](/en/contact-us)\n",
      "\n",
      "- [Search](javascript:void(0))\n",
      "\n",
      "# CeADAR\n",
      "\n",
      "## What is CeADAR\n",
      "\n",
      "CeADAR is the National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI in Ireland. CeADAR has more than 90 member companies ranging from multinationals to indigenous SMEs spanning every industry vertical. The primary work of the Centre is on cutting-edge applied AI innovation and developing and deploying industry prototypes and market-ready solutions to companies.\n",
      "\n",
      "We support companies in finding funding and investment, ecosystem networking in Ireland and Europe, training with and access to our powerful in-house supercomputer. CeADAR also helps its member companies benchmark their state of AI maturity and plan their AI strategy. CeADAR is very active in European funded projects, and in producing a succession of spinout companies.\n",
      "\n",
      "The Centre is the focal point of a thriving national ecosystem delivering frequent seminars, conferences, and members networking events throughout the year. CeADAR is one of only 9 Gold i-spaces in the EU and has won 2 national awards for contributions to the development and implementation of AI strategy to businesses in Ireland. The Centre contributes on various task forces in the OECD, the Department of Enterprise, national standards bodies, and business representative groups.\n",
      "\n",
      "**Expertise:**\n",
      "\n",
      "* Driving accelerated AI adoption in Irish enterprise\n",
      "* Creating and delivering value for clients\n",
      "* Strengthening client AI capability\n",
      "* Building sustained competitive advantage for clients\n",
      "* Supporting clients in finding investment\n",
      "* Fostering a strong national AI innovation ecosystem\n",
      "* Provision of AI training, upskilling and talent\n",
      "* Promotion of good AI governance and trustworthy AI\n",
      "\n",
      "**Research performed by:**\n",
      "\n",
      "* CeADAR in University College Dublin\n",
      "\n",
      "## Contact\n",
      "\n",
      "#### John Lonsdale\n",
      "\n",
      "Chief Executive Officer\n",
      "\n",
      "#### John Lonsdale\n",
      "\n",
      "Chief Executive Officer\n",
      "\n",
      "**Phone:** [+353 1 716 5716](tel:+35317165716)\n",
      "\n",
      "**Web:** [www.ceadar.ie](https://ceadar.ie/)\n",
      "\n",
      "**Twitter:** [@CeADARIreland](https://twitter.com/ceadarireland)\n",
      "\n",
      "[john.lonsdale@ucd.ie](mailto:john.lonsdale@ucd.ie)\n",
      "\n",
      " 2025 Enterprise Ireland All rights reserved\n",
      "\n",
      "VAT No: IE9590828H</raw></document>\n",
      "<document><title>CeADAR-Centre for Applied Data Analytics Research</title><url>https://stip.oecd.org/stip/interactive-dashboards/policy-initiatives/2025%2Fdata%2FpolicyInitiatives%2F200002846</url><content>The Centre for Applied Data Analytics Research (CeADAR) is Ireland's national centre for AI. A technology centre established to support</content></document>\n",
      "<document><title>CeADAR: Ireland's Centre for AI and Applied Data Analytics</title><url>https://www.clustercollaboration.eu/content/ceadar-irelands-centre-ai-and-applied-data-analytics</url><content># CeADAR: Ireland's Centre for AI and Applied Data Analytics CeADAR: Ireland's Centre for AI and Applied Data Analytics. The Centre for AI & Applied Data Analytics (CeADAR) is the national market-led technology centre for the research, development and deployment of AI & big data analytics technology and innovation, focusing on developing tools, techniques and technologies that enable people, organisations and industries to use AI & analytics for better decision making and competitive advantage. The aim of CeADAR is to rapidly prototype and deliver AI & analytics technology and solutions to industry from an agenda that is solely defined by the needs of the market. The Centres primary outputs are prototypes, demonstrators and bespoke solutions co-developed with individual industry members. The Centre has an extensive catalogue of technology demonstrators (>60), IP and AI/Machine Learning/Analytics technology reviews which are immediately available to members for evaluation.</content><raw>## Warning message\n",
      "\n",
      "Could not find appropriate formatter field to render *Primary Contact Telephone*.\n",
      "\n",
      "# CeADAR: Ireland's Centre for AI and Applied Data Analytics CeADAR: Ireland's Centre for AI and Applied Data Analytics\n",
      "\n",
      "Ireland: Eastern and Midland (Ireland)\n",
      "\n",
      "## Overview\n",
      "\n",
      "The Centre for AI & Applied Data Analytics (CeADAR) is the national market-led technology centre for the research, development and deployment of AI & big data analytics technology and innovation, focusing on developing tools, techniques and technologies that enable people, organisations and industries to use AI & analytics for better decision making and competitive advantage.\n",
      "\n",
      "The aim of CeADAR is to rapidly prototype and deliver AI & analytics technology and solutions to industry from an agenda that is solely defined by the needs of the market. The Centres primary outputs are prototypes, demonstrators and bespoke solutions co-developed with individual industry members.\n",
      "\n",
      "The Centre is also one the the EU's 30 Digital Innovation Hubs in AI, an accredited i-Space from the BDVA.\n",
      "\n",
      "Along with NextGeneration in Dublin,the Centre set up the European Data Science and AI Awards (The DatScis) which showcases and celebrates the talent and success of individuals and companies in AI across the EU  \n",
      " The CeADAR centre [www.ceadar.ie](http://www.ceadar.ie) has a high bandwidth, high speed data centre to enable the real-time processing of very large quantities of fast streaming data of the sort that is experienced in large industrial process control and prediction applications.CeADAR maintains its own data centre as well as using web services from various providers. Recently the Centre was awarded national funding to acquire a supercomputer which will also be accessible to the nation ecosystem of SMEs and MNCs  \n",
      " . The CeADAR Centre undertakes consultancy, contract research, participation in EU Horizon 2020 consortia, and develops up to 12 demonstrator projects per year in 2 cycles of 6 months each.\n",
      "\n",
      "Each project is proposed by the industry members and is focused onclose-to-market challenges.\n",
      "\n",
      "The Centre has an extensive catalogue of technology demonstrators (>60), IP and AI/Machine Learning/Analytics technology reviews which are immediately available to members for evaluation.  \n",
      " . Yes. service.\n",
      "\n",
      "##### Sectors\n",
      "\n",
      "Aeronautics & space - Agriculture - Agro-food - Construction & building sector - Consumer goods/products - Energy - Environment - Healthcare - Machinery - Manufacturing - Process Manufacturing - Professional Services - Retail & Wholesale - Transport & Logistics - Utilities and Oil & Gas\n",
      "\n",
      "##### Technology\n",
      "\n",
      "Monitoring and control - Smart Manufacturing / Industry 4.0 - Artificial intelligence - Natural Language Processing - Signal sensing - Video / image processing - Data analytics (predictive analysis, in-memory analytics, network analysis, advanced cluster analysis) - Data management (acquisition, extraction and integration) - Machine learning - Text & data mining - Block chain - IoT - Edge computing - Security and connectivity\n",
      "\n",
      "[Visit our website](http://www.ceadar.ie)\n",
      "\n",
      " </raw></document>\n",
      "<document><title>CeADAR and Equal1 Partner to Advance Ireland's ...</title><url>https://quantumcomputingreport.com/ceadar-and-equal1-partner-to-advance-irelands-quantum-ai-ecosystem/</url><content># CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem. CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem. Equal1, developer of Irelands first quantum computer *Bell-1*, has signed a Memorandum of Understanding (MoU) with CeADAR, Irelands Centre for Applied AI, to accelerate the national Quantum-AI infrastructure. The partnership targets joint development of funded R&D proposals and aims to build a framework supporting Irish business and academic adoption of hybrid quantum-AI systems. The collaboration will focus on integrating Equal1s compact quantum servers with AI workflows, leveraging CeADARs enterprise outreach to promote adoption. Equal1s approach, termed Quantum Computing 2.0, features a monolithic design where quantum processors and control electronics are integrated on a single chip, eliminating the need for bulky cryogenic systems. By combining technical innovation with ecosystem building, the CeADAREqual1 collaboration seeks to position Ireland as a leader in quantum-AI convergence, emphasizing strategic competitiveness in emerging technologies across Europe. The initiative will support national sovereignty in AI and quantum, foster talent development, and enable broader access to quantum-enhanced tools for industry.</content><raw>[Skip to content](#content)\n",
      "\n",
      "# CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem\n",
      "\n",
      "CeADAR and Equal1 Partner to Advance Irelands Quantum-AI Ecosystem\n",
      "\n",
      "[Equal1](https://www.equal1.com/), developer of Irelands first quantum computer *[Bell-1](https://www.equal1.com/bell-1)*, has signed a Memorandum of Understanding (MoU) with [CeADAR](https://ceadar.ie/), Irelands Centre for Applied AI, to accelerate the national Quantum-AI infrastructure. The partnership targets joint development of funded R&D proposals and aims to build a framework supporting Irish business and academic adoption of hybrid quantum-AI systems. Bell-1 uses a silicon-based architecture designed for scalability and cost-efficiency, signaling a transition from research prototypes to deployable systems.\n",
      "\n",
      "The collaboration will focus on integrating Equal1s compact quantum servers with AI workflows, leveraging CeADARs enterprise outreach to promote adoption. Equal1s approach, termed Quantum Computing 2.0, features a monolithic design where quantum processors and control electronics are integrated on a single chip, eliminating the need for bulky cryogenic systems. This architecture supports a smaller form factor and improved system stability, aligning with practical deployment needs for data-intensive environments.\n",
      "\n",
      "By combining technical innovation with ecosystem building, the CeADAREqual1 collaboration seeks to position Ireland as a leader in quantum-AI convergence, emphasizing strategic competitiveness in emerging technologies across Europe. The initiative will support national sovereignty in AI and quantum, foster talent development, and enable broader access to quantum-enhanced tools for industry.\n",
      "\n",
      "Full announcement available [here](https://www.equal1.com/post/ceadar-and-equal1-mou).\n",
      "\n",
      "May 9, 2025\n",
      "\n",
      "[Mohamed Abdel-Kareem](https://quantumcomputingreport.com/author/mohamed-abdel-kareem/ \"Posts by Mohamed Abdel-Kareem\")2025-05-09T20:28:05-07:00\n",
      "\n",
      "[Page load link](#)\n",
      "\n",
      "[Go to Top](#)\n",
      "\n",
      " </raw></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "CeADAR is Ireland's National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI. It supports over 90 member companies across various industries by driving applied AI innovation, developing and deploying industry prototypes and market-ready AI solutions. CeADAR helps companies find funding and investment, provides ecosystem networking in Ireland and Europe, offers training and access to a powerful in-house supercomputer, and assists members in benchmarking AI maturity and planning AI strategies. It is active in European projects, produces spinout companies, and is recognized as one of only 9 Gold i-spaces in the EU, having won national awards for AI strategy contributions. CeADAR also fosters a strong national AI innovation ecosystem and promotes good AI governance and trustworthy AI.\n",
      "\n",
      "**Source**  \n",
      "- https://www.enterprise-ireland.com/en/supports/become-more-innovative/technology-centres/ceadar (pages 1-3)  \n",
      "- https://www.clustercollaboration.eu/content/ceadar-irelands-centre-ai-and-applied-data-analytics\n",
      "('user', 'What is the CeADAR initiative in Ireland?')\n",
      "('assistant', \"CeADAR is Ireland's National Centre for Applied Artificial Intelligence and the European Digital Innovation Hub (EDIH) for AI. It supports over 90 member companies across various industries by driving applied AI innovation, developing and deploying industry prototypes and market-ready AI solutions. CeADAR helps companies find funding and investment, provides ecosystem networking in Ireland and Europe, offers training and access to a powerful in-house supercomputer, and assists members in benchmarking AI maturity and planning AI strategies. It is active in European projects, produces spinout companies, and is recognized as one of only 9 Gold i-spaces in the EU, having won national awards for AI strategy contributions. CeADAR also fosters a strong national AI innovation ecosystem and promotes good AI governance and trustworthy AI.\\n\\n**Source**  \\n- https://www.enterprise-ireland.com/en/supports/become-more-innovative/technology-centres/ceadar (pages 1-3)  \\n- https://www.clustercollaboration.eu/content/ceadar-irelands-centre-ai-and-applied-data-analytics\")\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Chunk 300 overlap 50\n",
    "inputs = GraphState(question=[HumanMessage(content=\"what is CeADAR, Ireland?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8edea5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the AIME 2024 pass@1 value for QwQ-32B-Preview?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>pass@1 = 1 / k *  (pi), where pi denotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n",
      "\n",
      "1https://aider.chat\n",
      "\n",
      "2https://codeforces.com\n",
      "\n",
      "3https://www.cms.org.cn/Home/comp/comp/cid/12.html</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "<document><content># Evaluation Setup\n",
      "\n",
      "We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@ evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top- value of 0.95 to generate  responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "<document><content>- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content># Figure 2\n",
      "\n",
      "AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>AIME 2024\n",
      "# Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\n",
      "\n",
      "| Model                        | pass\\@1 | cons\\@64 | pass\\@1 | pass\\@1 | pass\\@1 |\n",
      "| ---------------------------- | ------- | -------- | ------- | ------- | ------- |\n",
      "| QwQ-32B-Preview              | 50.0    | 60.0     | 90.6    | 54.5    | 41.9    |\n",
      "| DeepSeek-R1-Zero-Qwen-32B    | 47.0    | 60.0     | 91.6    | 55.0    | 40.2    |\n",
      "| DeepSeek-R1-Distill-Qwen-32B | 72.6    | 83.3     | 94.3    | 62.1    | 57.2    |</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content>DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content># 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.\n",
      "\n",
      "|   |\n",
      "| - |</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGos core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>| QwQ-32B-Preview               | 50.0      | 60.0 | 90.6     | 54.5 | 41.9             | 1316             |\n",
      "| DeepSeek-R1-Distill-Qwen-1.5B | 28.9      | 52.7 | 83.9     | 33.8 | 16.9             | 954              |\n",
      "| DeepSeek-R1-Distill-Qwen-7B   | 55.5      | 83.3 | 92.8     | 49.1 | 37.6             | 1189             |\n",
      "| DeepSeek-R1-Distill-Qwen-14B  | 69.7      | 80.0 | 93.9     | 59.1 | 53.1             | 1481             |\n",
      "| DeepSeek-R1-Distill-Qwen-32B  | 72.6      | 83.3 | 94.3     | 62.1 | 57.2             | 1691             |\n",
      "| DeepSeek-R1-Distill-Llama-8B  | 50.4      | 80.0 | 89.1     | 49.0 | 39.6             | 1205             |</content><source>data/Deepseek-r1.pdf</source><page>14</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "The AIME 2024 pass@1 value for QwQ-32B-Preview is 50.0%.\n",
      "\n",
      "**Source:**\n",
      "- data/Deepseek-r1.pdf (page 14)\n",
      "('user', 'What is the AIME 2024 pass@1 value for QwQ-32B-Preview?')\n",
      "('assistant', 'The AIME 2024 pass@1 value for QwQ-32B-Preview is 50.0%.\\n\\n**Source:**\\n- data/Deepseek-r1.pdf (page 14)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Chunk 700 overlap 150\n",
    "inputs = GraphState(question=[HumanMessage(content=\"What is value of QwQ-32B-Preview's AIME 2024 pass@1?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the pass@1 value for QwQ-32B-Preview's AIME 2024?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>pass@1 = 1 / k *  (pi), where pi denotes the correctness of the i-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n",
      "\n",
      "1https://aider.chat</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "<document><content>as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>| AIME 2024 (Pass\\@1)             | 16.0                    | 9.3    | 39.2     | 63.6   | 79.2   | 79.8     |      |\n",
      "| Math                            | MATH-500 (Pass\\@1)      | 78.3   | 74.6     | 90.2   | 90.0   | 96.4     | 97.3 |</content><source>data/Deepseek-r1.pdf</source><page>13</page></document>\n",
      "<document><content>as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "<document><content>we default to pass@ evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top- value of 0.95 to generate  responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "<document><content>its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "The pass@1 value for QwQ-32B-Preview on AIME 2024 is 39.2%.\n",
      "\n",
      "**Source**  \n",
      "- data/Deepseek-r1.pdf (page 13)\n",
      "('user', \"What is the pass@1 value for QwQ-32B-Preview's AIME 2024?\")\n",
      "('assistant', 'The pass@1 value for QwQ-32B-Preview on AIME 2024 is 39.2%.\\n\\n**Source**  \\n- data/Deepseek-r1.pdf (page 13)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Chunk 300 overlap 50\n",
    "inputs = GraphState(question=[HumanMessage(content=\"What is value of QwQ-32B-Preview's AIME 2024 pass@1?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "005406e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = app.get_state(config).values\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776998b4",
   "metadata": {},
   "source": [
    "## Difficult Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "23034810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the key differences in training strategies between DeepSeek-R1-Zero and DeepSeek-R1?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content># 2.3.1. Cold Start\n",
      "\n",
      "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators.</content><source>data/Deepseek-r1.pdf</source><page>9</page></document>\n",
      "<document><content>DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths</content><source>data/Deepseek-r1.pdf</source><page>13</page></document>\n",
      "<document><content>Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n",
      "\n",
      "DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning</content><source>data/Deepseek-r1.pdf</source><page>1</page></document>\n",
      "<document><content># 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.\n",
      "\n",
      "|   |\n",
      "| - |</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content># 2.2.3. Training Template\n",
      "\n",
      "To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process.\n",
      "\n",
      "# 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content>DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.</content><source>data/Deepseek-r1.pdf</source><page>1</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "The key differences in training strategies between DeepSeek-R1-Zero and DeepSeek-R1 are:\n",
      "\n",
      "- **DeepSeek-R1-Zero** is trained via large-scale reinforcement learning (RL) directly from the base model without any supervised fine-tuning (SFT) or cold-start data. It naturally learns reasoning capabilities through RL alone but suffers from issues like poor readability and language mixing.\n",
      "\n",
      "- **DeepSeek-R1** incorporates a multi-stage training pipeline including a cold-start phase before RL. This involves collecting a small amount of long chain-of-thought (CoT) data to fine-tune the base model initially, using methods like few-shot prompting, direct prompting for detailed answers, and human-annotated refinements. After this cold start, reasoning-oriented RL is applied, followed by additional supervised fine-tuning using rejection sampling on RL checkpoints combined with supervised data in domains like writing and factual QA.\n",
      "\n",
      "This multi-stage approach in DeepSeek-R1 improves reasoning performance, readability, and generalization compared to DeepSeek-R1-Zero.\n",
      "\n",
      "**Source:**\n",
      "- data/Deepseek-r1.pdf (pages 1, 3, 8, 9)\n",
      "('user', 'What are the key differences in training strategies between DeepSeek-R1-Zero and DeepSeek-R1?')\n",
      "('assistant', 'The key differences in training strategies between DeepSeek-R1-Zero and DeepSeek-R1 are:\\n\\n- **DeepSeek-R1-Zero** is trained via large-scale reinforcement learning (RL) directly from the base model without any supervised fine-tuning (SFT) or cold-start data. It naturally learns reasoning capabilities through RL alone but suffers from issues like poor readability and language mixing.\\n\\n- **DeepSeek-R1** incorporates a multi-stage training pipeline including a cold-start phase before RL. This involves collecting a small amount of long chain-of-thought (CoT) data to fine-tune the base model initially, using methods like few-shot prompting, direct prompting for detailed answers, and human-annotated refinements. After this cold start, reasoning-oriented RL is applied, followed by additional supervised fine-tuning using rejection sampling on RL checkpoints combined with supervised data in domains like writing and factual QA.\\n\\nThis multi-stage approach in DeepSeek-R1 improves reasoning performance, readability, and generalization compared to DeepSeek-R1-Zero.\\n\\n**Source:**\\n- data/Deepseek-r1.pdf (pages 1, 3, 8, 9)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = GraphState(question=[HumanMessage(content=\"What is the main difference between DeepSeek-R1-Zero and DeepSeek-R1 in terms of training strategy?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "531a1e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Group Relative Policy Optimization (GRPO), and what is its purpose in DeepSeek-R1-Zero?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content># Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>- Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning</content><source>data/Deepseek-r1.pdf</source><page>1</page></document>\n",
      "<document><content>which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.</content><source>data/Deepseek-r1.pdf</source><page>1</page></document>\n",
      "<document><content>For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar</content><source>data/Deepseek-r1.pdf</source><page>13</page></document>\n",
      "<document><content># 2.2.1. Reinforcement Learning Algorithm\n",
      "\n",
      "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question , GRPO samples a group of outputs {1, 2,    , } from the old policy  and then optimizes the policy model  by maximizing the following objective:\n",
      "\n",
      "JGRPO() = E[   (), {}    (|)]\n",
      "\n",
      "1          min  ( |)          , clip   ( |)    , 1  , 1 +     DKL  ||ref ,</content><source>data/Deepseek-r1.pdf</source><page>5</page></document>\n",
      "<document><content>DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.</content><source>data/Deepseek-r1.pdf</source><page>5</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm used in DeepSeek-R1-Zero to reduce training costs by eliminating the need for a separate critic model. Instead, GRPO estimates the baseline from group scores by sampling a group of outputs from the old policy and optimizing the policy model to maximize a clipped objective with a KL divergence penalty relative to a reference policy. Its purpose in DeepSeek-R1-Zero is to efficiently train the model via large-scale RL, enabling it to develop robust reasoning capabilities without supervised fine-tuning.\n",
      "\n",
      "**Source:**\n",
      "- data/Deepseek-r1.pdf (page 5)\n",
      "('user', 'What is Group Relative Policy Optimization (GRPO), and what is its purpose in DeepSeek-R1-Zero?')\n",
      "('assistant', 'Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm used in DeepSeek-R1-Zero to reduce training costs by eliminating the need for a separate critic model. Instead, GRPO estimates the baseline from group scores by sampling a group of outputs from the old policy and optimizing the policy model to maximize a clipped objective with a KL divergence penalty relative to a reference policy. Its purpose in DeepSeek-R1-Zero is to efficiently train the model via large-scale RL, enabling it to develop robust reasoning capabilities without supervised fine-tuning.\\n\\n**Source:**\\n- data/Deepseek-r1.pdf (page 5)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = GraphState(question=[HumanMessage(content=\"What is GRPO (Group Relative Policy Optimization), and why is it used in DeepSeek-R1-Zero?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c7323108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the two types of rewards used in training DeepSeek-R1-Zero, and what purposes do they serve?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n",
      "\n",
      "# 2.2.3. Training Template</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content># 2.3.1. Cold Start\n",
      "\n",
      "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators.</content><source>data/Deepseek-r1.pdf</source><page>9</page></document>\n",
      "<document><content>RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content>DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths</content><source>data/Deepseek-r1.pdf</source><page>13</page></document>\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n",
      "\n",
      "DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.</content><source>data/Deepseek-r1.pdf</source><page>8</page></document>\n",
      "<document><content>DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content># 2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\n",
      "\n",
      "Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\n",
      "\n",
      "# 2.3.1. Cold Start</content><source>data/Deepseek-r1.pdf</source><page>9</page></document>\n",
      "<document><content>- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "no\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mweb_search\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<document><title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...</title><url>https://arxiv.org/pdf/2501.12948</url><content>by D Guo  2025  Cited by 5616  To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:  Accuracy rewards: The</content><raw>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "AIME 2024 (Pass@1) Codeforces (Percentile) GPQA Diamond (Pass@1) MATH-500 (Pass@1) MMLU (Pass@1) SWE-bench Verified (Resolved) 0 20 40 60 80 100 Accuracy / Percentile (%) 79.8 96.3 71.5 97.3 90.8 49.2 79.2 96.6 75.7 96.4 91.8 48.9 72.6 90.6 62.1 94.3 87.4 36.8 63.6 93.4 60.0 90.0 85.2 41.6 39.2 58.7 59.1 90.2 88.5 42.0 DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 Figure 1 | Benchmark performance of DeepSeek-R1.\n",
      "arXiv:2501.12948v1 [cs.CL] 22 Jan 2025 Contents 1 Introduction 3 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "4 1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "4 2 Approach 5 2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5 2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .\n",
      "5 2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . .\n",
      "5 2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6 2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6 2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6 2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .\n",
      "9 2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9 2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .\n",
      "10 2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .\n",
      "10 2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . .\n",
      "11 2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . .\n",
      "11 3 Experiment 11 3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "13 3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "14 4 Discussion 14 4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "14 4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "15 5 Conclusion, Limitations, and Future Work 16 A Contributions and Acknowledgments 20 2 1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI).\n",
      "Recently, post-training has emerged as an important component of the full training pipeline.\n",
      "It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAIs o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAIs o1 series models.\n",
      "In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\n",
      "During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.\n",
      "However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\n",
      "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\n",
      "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.\n",
      "3 1.1. Contributions Post-Training: Large-Scale Reinforcement Learning on the Base Model  We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n",
      " We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human pref-erences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models.\n",
      "Distillation: Smaller Models Can Be Powerful Too  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.\n",
      " Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n",
      "1.2. Summary of Evaluation Results  Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\n",
      " Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n",
      "4  Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\n",
      "Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n",
      "2. Approach 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models.\n",
      "2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.\n",
      "2.2.1. Reinforcement Learning Algorithm Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.\n",
      "Specifically, for each question , GRPO samples a group of outputs {1, 2,    , } from the old policy and then optimizes the policy model by maximizing the following objective: J () = E[(), {} =1 (|)] 1    =1 \u0012 min \u0012 (|) (|) , clip \u0012 (|) (|) , 1 , 1 +  \u0013  \u0013 D \u0000|| \u0001\u0013 , (1) D \u0000|| \u0001 = (|) (|) log (|) (|) 1, (2) where and are hyper-parameters, and is the advantage, computed using a group of rewards {1, 2, . . . , } corresponding to the outputs within each group: = m({1, 2,    , }) s({1, 2,    , }) .\n",
      "(3) 5 A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
      "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant: Table 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training.\n",
      "2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL.\n",
      "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:  Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\n",
      "For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n",
      " Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between <think> and </think> tags.\n",
      "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.\n",
      "2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\n",
      "We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strate-giesto ensure that we can accurately observe the models natural progression during the RL process.\n",
      "2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.\n",
      "Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers 6 Model AIME 2024 MATH-500 GPQA LiveCode CodeForces Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 OpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843 DeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444 Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks.\n",
      "Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.\n",
      "DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n",
      "Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n",
      "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-7 Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\n",
      "ment throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-tation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.\n",
      "One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n",
      "Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n",
      "This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n",
      "8 Question: If > 1, then the sum of the real solutions of   + = is equal to Response: <think> To solve the equation   + = , lets start by squaring both    \u0010  +  \u00112 = 2 = + = 2.\n",
      "Rearrange to isolate the inner square root term: (2)2 = + =2 22 + (2)2 = + =4 22 + (2 ) = 0 . . .\n",
      "Wait, wait. Wait. Thats an aha moment I can flag here.\n",
      "Lets reevaluate this step-by-step to identify if the correct sum can be    We started with the equation:   + =  First, lets square both sides:  + = 2 = + = 2 Next, I could square both sides again, treating the equation:    . . .\n",
      "Table 3 | An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.\n",
      "Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data.\n",
      "2.3. DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows.\n",
      "2.3.1. Cold Start Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators.\n",
      "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data 9 include:  Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special_token|<reasoning_process>|special_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.\n",
      " Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models.\n",
      "2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.\n",
      "2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.\n",
      "Reasoning data We curate reasoning prompts and generate reasoning trajectories by perform-ing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\n",
      "Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n",
      "10 Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as hello we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.\n",
      "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.\n",
      "2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the models helpfulness and harmlessness while simultane-ously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n",
      "2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n",
      "For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.\n",
      "3. Experiment Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 11 2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08  2025-01), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n",
      "Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark.\n",
      "Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\n",
      "Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-mance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a).\n",
      "Evaluation Setup We set the maximum generation length to 32,768 tokens for the models.\n",
      "We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\n",
      "Specifically, we use a sampling temperature of 0.6 and a top-value of 0.95 to generate  responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as pass@1 = 1    =1 , where denotes the correctness of the -th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64.\n",
      "1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 12 3.1. DeepSeek-R1 Evaluation Benchmark (Metric) Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Sonnet-1022 0513 V3 o1-mini o1-1217 R1 Architecture --MoE --MoE # Activated Params --37B --37B # Total Params --671B --671B English MMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8 MMLU-Redux (EM) 88.9 88.0 89.1 86.7 -92.9 MMLU-Pro (EM) 78.0 72.6 75.9 80.3 -84.0 DROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2 IF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 -83.3 GPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5 SimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1 FRAMES (Acc.) 72.5 80.5 73.3 76.9 -82.5 AlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 -87.6 ArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 -92.3 Code LiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9 Codeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3 Codeforces (Rating) 717 759 1134 1820 2061 2029 SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3 Math AIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8 MATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 CNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 -78.8 Chinese CLUEWSC (EM) 85.4 87.9 90.9 89.9 -92.8 C-Eval (EM) 76.7 76.0 86.5 68.9 -91.8 C-SimpleQA (Correct) 55.4 58.7 68.0 40.3 -63.7 Table 4 | Comparison between DeepSeek-R1 and other representative models.\n",
      "For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-provement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-icant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%.\n",
      "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n",
      "On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n",
      "3.2. Distilled Model Evaluation Model AIME 2024 MATH-500 GPQA LiveCode CodeForces Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759 Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717 OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316 DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954 DeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189 DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691 DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205 DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633 Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.\n",
      "As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-tion. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n",
      "4. Discussion 4.1. Distillation v.s. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n",
      "To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale 14 Model AIME 2024 MATH-500 GPQA Diamond LiveCodeBench pass@1 cons@64 pass@1 pass@1 pass@1 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 DeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 Table 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\n",
      "RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.\n",
      "Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning.\n",
      "4.2. Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n",
      "Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not con-ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n",
      "Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process.\n",
      "However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an 15 exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process.\n",
      "Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGos core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.\n",
      "In conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge.\n",
      "5. Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n",
      "We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n",
      "In the future, we plan to invest in research across the following directions for DeepSeek-R1.\n",
      " General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output.\n",
      "Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n",
      " Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n",
      " Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n",
      " Software Engineering Tasks: Due to the long evaluation times, which impact the effi-ciency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.\n",
      "16 References AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md.\n",
      "Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet.\n",
      "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\n",
      "URL https://arxiv.org/abs/2107.03374.\n",
      "A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n",
      "Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\n",
      "X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179.\n",
      "L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760.\n",
      "A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.\n",
      "Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024.\n",
      "Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.\n",
      "D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n",
      "Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n",
      "N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\n",
      "Livecodebench: Holistic and contamination free evaluation of large language models for code.\n",
      "CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\n",
      "17 S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\n",
      "Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.\n",
      "A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\n",
      "H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-ing massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.\n",
      "T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024.\n",
      "H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n",
      "B. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval.\n",
      "MAA.\n",
      "American invitational mathematics examination - aime.\n",
      "In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime.\n",
      "OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\n",
      "OpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin g-to-reason-with-llms/.\n",
      "OpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing -simpleqa/.\n",
      "OpenAI. Introducing SWE-bench verified were releasing a human-validated subset of swe-bench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/.\n",
      "Qwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm .github.io/blog/qwq-32b-preview/.\n",
      "Qwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5.\n",
      "D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\n",
      "GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\n",
      "Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n",
      "D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a. URL http://arxiv.org/abs/1712.01815.\n",
      "18 D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354359, 2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\n",
      "C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033 14.\n",
      "T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\n",
      "J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\n",
      "P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023.\n",
      "X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\n",
      "Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
      "Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.\n",
      "URL https://doi.org/10.48550/arXiv.2406.01574.\n",
      "C. S. Xia, Y. Deng, S. Dunn, and L. Zhang.\n",
      "Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.\n",
      "H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152.\n",
      "J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n",
      "19 Appendix A. Contributions and Acknowledgments Core Contributors Daya Guo Dejian Yang Haowei Zhang Junxiao Song Ruoyu Zhang Runxin Xu Qihao Zhu Shirong Ma Peiyi Wang Xiao Bi Xiaokang Zhang Xingkai Yu Yu Wu Z.F. Wu Zhibin Gou Zhihong Shao Zhuoshu Li Ziyi Gao Contributors Aixin Liu Bing Xue Bingxuan Wang Bochao Wu Bei Feng Chengda Lu Chenggang Zhao Chengqi Deng Chong Ruan Damai Dai Deli Chen Dongjie Ji Erhang Li Fangyun Lin Fucong Dai Fuli Luo* Guangbo Hao Guanting Chen Guowei Li H. Zhang Hanwei Xu Honghui Ding Huazuo Gao Hui Qu Hui Li Jianzhong Guo Jiashi Li Jingchang Chen Jingyang Yuan Jinhao Tu Junjie Qiu Junlong Li J.L. Cai Jiaqi Ni Jian Liang Jin Chen Kai Dong Kai Hu* Kaichao You Kaige Gao Kang Guan Kexin Huang Kuai Yu Lean Wang Lecong Zhang Liang Zhao Litong Wang Liyue Zhang Lei Xu Leyi Xia Mingchuan Zhang Minghua Zhang Minghui Tang Mingxu Zhou Meng Li Miaojun Wang Mingming Li Ning Tian Panpan Huang Peng Zhang Qiancheng Wang Qinyu Chen Qiushi Du Ruiqi Ge* Ruisong Zhang Ruizhe Pan Runji Wang R.J. Chen R.L. Jin 20 Ruyi Chen Shanghao Lu Shangyan Zhou Shanhuang Chen Shengfeng Ye Shiyu Wang Shuiping Yu Shunfeng Zhou Shuting Pan S.S. Li Shuang Zhou Shaoqing Wu Shengfeng Ye Tao Yun Tian Pei Tianyu Sun T. Wang Wangding Zeng Wen Liu Wenfeng Liang Wenjun Gao Wenqin Yu* Wentao Zhang W.L. Xiao Wei An Xiaodong Liu Xiaohan Wang Xiaokang Chen Xiaotao Nie Xin Cheng Xin Liu Xin Xie Xingchao Liu Xinyu Yang Xinyuan Li Xuecheng Su Xuheng Lin X.Q. Li Xiangyue Jin Xiaojin Shen Xiaosha Chen Xiaowen Sun Xiaoxiang Wang Xinnan Song Xinyi Zhou Xianzu Wang Xinxia Shan Y.K. Li Y.Q. Wang Y.X. Wei Yang Zhang Yanhong Xu Yao Li Yao Zhao Yaofeng Sun Yaohui Wang Yi Yu Yichao Zhang Yifan Shi Yiliang Xiong Ying He Yishi Piao Yisong Wang Yixuan Tan Yiyang Ma* Yiyuan Liu Yongqiang Guo Yuan Ou Yuduan Wang Yue Gong Yuheng Zou Yujia He Yunfan Xiong Yuxiang Luo Yuxiang You Yuxuan Liu Yuyang Zhou Y.X. Zhu Yanping Huang Yaohui Li Yi Zheng Yuchen Zhu Yunxian Ma Ying Tang Yukun Zha Yuting Yan Z.Z. Ren Zehui Ren Zhangli Sha Zhe Fu Zhean Xu Zhenda Xie Zhengyan Zhang Zhewen Hao Zhicheng Ma Zhigang Yan Zhiyu Wu Zihui Gu 21 Zijia Zhu Zijun Liu* Zilin Li Ziwei Xie Ziyang Song Zizheng Pan Zhen Huang Zhipeng Xu Zhongyu Zhang Zhen Zhang Within each role, authors are listed alphabetically by the first name. Names marked with * denote individuals who have departed from our team.\n",
      "22</raw></document>\n",
      "<document><title>DeepSeek R1 and R1-Zero Explained</title><url>https://thelmbook.com/articles/#!./DeepSeek-R1.md</url><content>At its core, the system uses two primary types of rewards: accuracy rewards and format rewards: \\\"The accuracy reward model evaluates whether the response is</content></document>\n",
      "<document><title>How DeepSeek R1 Works: Explaining All Its Key ...</title><url>https://www.pedromebo.com/blog/en-how-deepseek-r1-works</url><content>To train DeepSeek-R1, the following types of rewards are defined: Accuracy Rewards: ... Format Rewards: ... No \\\"Outcome\\\" or \\\"Process Neural Reward</content><raw>![deepseek_r1_learn_banner](https://res.cloudinary.com/pedromebo/image/upload/q_auto,f_auto,c_fill,ar_5:2,w_1200/pedromebo/banner/deepseek_r1_learn_banner \"deepseek_r1_learn_banner\")\n",
      "\n",
      "# How DeepSeek R1 Works: Explaining All Its Key Components and Their Consequences\n",
      "\n",
      "Written on February 03, 2025 by Pedro Medinilla.\n",
      "\n",
      "**DeepSeek-R1** is a family of large-scale language models (LLMs) whose primary goal is to enhance reasoning capabilities and generate responses that are more aligned with human context. To achieve this, it relies on various stages of **Reinforcement Learning (RL)** and, optionally, **Supervised Fine-Tuning (SFT)**. One of its variants, **DeepSeek-R1-Zero**, demonstrates that it is possible to develop a reasoning model solely through RL techniques, even without a prior supervised fine-tuning phase. This paves the way for new approaches to training LLMs, reducing dependence on large volumes of labeled data.\n",
      "\n",
      "This article explores how the use of Chain of Thought reasoning, Reward Modeling configuration, and the RL process itself have contributed to DeepSeek-R1 outstanding performance metrics. It also delves into the phenomenon of the aha moment and how, through distillation, the acquired reasoning can be transferred to smaller models without losing effectiveness.\n",
      "\n",
      "## Types of DeepSeek R1 Models\n",
      "\n",
      "On January 20, DeepSeek released its DeepSeek-R1 and DeepSeek-R1-Zero modelstwo new reasoning models that outperform OpenAI o1 in some benchmarks.\n",
      "\n",
      "In the words of the developers ([Excerpt from the paper](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)):\n",
      "\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguingreasoning behaviors. However, it encounters challenges such as poor readability, and languagemixing. To address these issues and further enhance reasoning performance, we introduceDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support theresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n",
      "\n",
      "Here are the main differences between them:\n",
      "\n",
      "### Training\n",
      "\n",
      "### Strengths\n",
      "\n",
      "### Weaknesses\n",
      "\n",
      "## How Does DeepSeek R1 Work?\n",
      "\n",
      "### Chain of Thought: Structuring Thought Processes\n",
      "\n",
      "The concept of Chain of Thought (CoT) involves revealing, at least internally, the reasoning steps the model takes before providing a final answer. This process is fundamental to DeepSeek-R1 approach because:\n",
      "\n",
      "Essentially, this is a Prompt Engineering technique that enables the model to generate more coherent and accurate responses while also making its reasoning process easier to refine and adjust.\n",
      "\n",
      "It is important not to confuse system prompting with Chain of Thought. System prompting refers to a training technique that involves guiding the model through a series of steps to generate a coherent response, while the latter refers to the internal structure of the reasoning steps the model follows before producing a final answer.\n",
      "\n",
      "This is the prompt template DeepSeek R1 has used to achieve this, as shown in their paper. Additionally, responses are labeled into two categories: think and answer.\n",
      "\n",
      "`A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
      "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
      "with the answer. The reasoning process and answer are enclosed within <think> </think> and\n",
      "<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
      "<answer> answer here </answer>. User: **prompt**. Assistant:`\n",
      "\n",
      "### Example of Chain of Thought for the question Are there more doors or chairs in the world?:\n",
      "\n",
      "Response from o1:\n",
      "\n",
      "`<think>\n",
      "1. There are millions of buildings in the world, and each one can have multiple internal and external doors.\n",
      "2. At the same time, almost all houses, offices, and schools contain multiple chairs.\n",
      "3. In a typical house, there may be an average of 5 to 10 doors (main entrance, interior doors, closets, etc.), while the same house might have an average of 4 to 8 chairs.\n",
      "4. However, there are also commercial spaces and hotels with many doors, as well as places with many seats (stadiums, restaurants).\n",
      "5. Many large buildings can have hundreds of doors, but also hundreds of chairs (conference rooms, cafeterias, etc.).\n",
      "6. Comparing these totals is complex, but most informal estimates tend to suggest that there are more doors, considering that even closet doors, vehicle doors, and other types of doors count.\n",
      "7. It could be very close, but popular opinion leans towards the idea that there might be more doors than chairs, given that every home and every car adds a significant number of doors.\n",
      "</think>`\n",
      "`Final answer: There are possibly more doors than chairs in the world.`\n",
      "\n",
      "Response from DeepSeek R1:\n",
      "\n",
      "`<think>\n",
      " \n",
      "Okay, so the question is whether there are more doors or chairs in the world. \n",
      "Hmm, interesting. Let me try to break this down. \n",
      " \n",
      "First, I need to think about where doors and chairs are commonly found. Doors \n",
      "are part of buildings, right? Every house, apartment, office, school, hospital, \n",
      "etc., has multiple doors. Even things like closets, bathrooms, and cabinets have \n",
      "doors. So, in a typical house, there might be several doors. For example, maybe \n",
      "each room has a door, plus doors for the front and back, maybe a garage door, \n",
      "and then all the little doors on cabinets and wardrobes. That adds up. \n",
      " \n",
      "Chairs, on the other hand, are also pretty common. Every home has chairs around \n",
      "the dining table, maybe a few in the living room, offices have chairs for desks, \n",
      "schools have chairs in classrooms, restaurants, theaters, stadiums... But wait, \n",
      "some places might have more chairs than doors. Like a movie theater has a lot of \n",
      "seats (which are chairs) but maybe only a few doors. Similarly, a school \n",
      "classroom might have 30 chairs but just one or two doors. So maybe chairs are \n",
      "more concentrated in certain areas. \n",
      " \n",
      "But then again, think about all the cabinets, closets, and small compartments \n",
      "that have doors. Each kitchen might have a dozen cabinet doors. In a hotel, \n",
      "every room has a door, and each room might have a couple of chairs. But maybe \n",
      "hotels have more doors because each room has a door, and each bathroom within \n",
      "the room has a door, plus closet doors. So for each guest room, maybe 3-4 doors \n",
      "but only 1-2 chairs. So in that case, doors might outnumber chairs. \n",
      " \n",
      "Also, consider vehicles. Cars have doors (usually 4 per car) and seats. But \n",
      "seats in cars are chairs? Maybe. So a car has 4 doors and, say, 5 seats. So \n",
      "here, chairs (seats) outnumber doors. But how many cars are there globally? \n",
      "There are over a billion cars, so that's a lot of seats. But then, trucks, \n",
      "buses, trains, airplanesall have seats and doors. So maybe in vehicles, chairs \n",
      "(seats) are more numerous than doors. For example, a bus might have 2 doors but \n",
      "40 seats. So that's a big difference. \n",
      " \n",
      "But then again, in terms of buildings, especially residential buildings, maybe \n",
      "doors are more. For example, a typical house might have 10 doors (front, back, \n",
      "bedrooms, bathroom, closets, cabinets) and maybe 6 chairs. So doors would \n",
      "outnumber chairs there. But in an office building, maybe each office has a \n",
      "door, but each desk has a chair. So if there are many offices with doors and \n",
      "each person has a chair, maybe chairs outnumber doors there. Hmm. \n",
      " \n",
      "Then there's the question of industrial and commercial spaces. Factories might \n",
      "have large doors for loading docks but also many chairs in break rooms or \n",
      "offices. But maybe not as many chairs as doors? Or maybe more? \n",
      " \n",
      "Also, think about public infrastructure. Public restrooms have multiple stalls \n",
      "with doors. Each stall has a door, but how many chairs? Maybe just one or none. \n",
      "So in restrooms, doors (stall doors) might outnumber chairs. \n",
      " \n",
      "Another angle: consider temporary chairs. Events like concerts or conferences \n",
      "set up temporary chairs, but those are not permanent. Do those count? The \n",
      "question is about the world, so maybe all existing chairs, including temporary \n",
      "ones. But doors are generally permanent fixtures. However, temporary structures \n",
      "might have temporary doors as well, like portable toilets or pop-up shops. Not \n",
      "sure if that's significant. \n",
      " \n",
      "Also, in many parts of the world, especially in developing countries, some \n",
      "homes might have fewer chairs. For example, in some cultures, people might sit \n",
      "on the floor more, so fewer chairs. But they would still have doors. So that \n",
      "might tip the balance towards doors. \n",
      " \n",
      "Additionally, think about all the appliances and furniture that have small \n",
      "doors, like refrigerators, microwaves, ovens, washing machines. Each of those \n",
      "has a door. So even in a kitchen, you might have several appliance doors plus \n",
      "cabinet doors. Each of those counts as a door. Meanwhile, chairs are only the \n",
      "seats people sit on. \n",
      " \n",
      "Wait, but chairs can be stacked or stored. For example, a school might have a \n",
      "storage room with hundreds of chairs for events. So even if they're not in use, \n",
      "they exist. Similarly, restaurants might have extra chairs. So chairs can be \n",
      "stored in large numbers. \n",
      " \n",
      "But doors are part of the structure. Once a building is built, the doors are \n",
      "fixed. Unless you have a lot of cabinets and closets, which many buildings do. \n",
      "So maybe the sheer number of cabinet doors in homes and offices adds up to a \n",
      "huge number. \n",
      " \n",
      "Let me try to estimate roughly. Let's say the global population is around 8 \n",
      "billion. Assume each person has a certain number of doors and chairs in their \n",
      "living environment. \n",
      " \n",
      "For doors: If each person has, say, 10 doors (considering homes, workplaces, \n",
      "schools, etc.), that would be 80 billion doors. But that might be high. \n",
      "Alternatively, think of the number of buildings. There are roughly 2 billion \n",
      "households worldwide. If each household has, on average, 10 doors (including \n",
      "interior doors, cabinets, etc.), that's 20 billion doors. Then add doors from \n",
      "commercial buildings, offices, schools, hospitals, vehicles, etc. Maybe the \n",
      "total is in the tens of billions. \n",
      " \n",
      "For chairs: Each household might have, say, 6 chairs. So 2 billion households \n",
      "would have 12 billion chairs. Then add chairs in offices (each office worker \n",
      "has a chair), schools (each student has a chair), public places like \n",
      "restaurants, theaters, etc. Also, vehicles contribute seats as chairs. If there \n",
      "are over a billion cars, each with 5 seats, that's 5 billion chairs. Then \n",
      "buses, trains, planes add more. Maybe total chairs are around 20-30 billion? \n",
      "Not sure. \n",
      " \n",
      "If chairs are around 25 billion, then doors would be more. But I'm not sure if \n",
      "my estimates are accurate. Maybe cabinet doors are overcounted. Do we consider \n",
      "small cabinet doors as \"doors\" in this context? The question is in Spanish, \n",
      "\"puertas\", which typically refers to larger doors like room doors, but maybe it \n",
      "includes any door, even small ones. If yes, then cabinet doors count, which \n",
      "would significantly increase the number. \n",
      " \n",
      "Alternatively, maybe the original question is considering only doors that lead \n",
      "into rooms or buildings, not small cabinet doors. That would change the \n",
      "calculation. But the user didn't specify, so I have to assume all types of \n",
      "doors. \n",
      " \n",
      "In conclusion, it's a close call, but considering all types of doors (including \n",
      "cabinets, appliances, vehicles), doors probably outnumber chairs globally. \n",
      "</think>`\n",
      "`Final answer: \n",
      "There are probably more doors than chairs in the world, thanks to their presence in large and small structures, vehicles, and appliances.`\n",
      "\n",
      "This *step-by-step reasoning* demonstrates how logic is broken down before reaching an answer, which is fundamental for RL-based training.\n",
      "\n",
      "### Reinforcement Learning\n",
      "\n",
      "**DeepSeek-R1-Zero** exemplifies the direct application of RL to the base model without a prior supervised fine-tuning stage. The approach is based on the idea that, with the right incentives (rewards for accuracy and format), the model:\n",
      "\n",
      "Key points of the GRPO algorithm:\n",
      "\n",
      "![GRPO Algorithm](https://i.gyazo.com/fd8cd4eaded094836dec486108e3d0b2.png)\n",
      "\n",
      "![GRPO Algorithm](https://i.gyazo.com/fd8cd4eaded094836dec486108e3d0b2.png)\n",
      "\n",
      "### Reward Modeling\n",
      "\n",
      "To train DeepSeek-R1, the following types of rewards are defined:\n",
      "\n",
      "**Accuracy Rewards**:\n",
      "\n",
      "**Format Rewards**:\n",
      "\n",
      "No \"Outcome\" or \"Process Neural Reward Model\" is employed in DeepSeek-R1-Zero's initial stage to prevent reward hacking and the complexity of training an additional neural network.\n",
      "\n",
      "### Distillation\n",
      "\n",
      "The distillation process aims to transfer the reasoning capabilities acquired by DeepSeek-R1 (or other large models) to smaller models while maintaining performance:\n",
      "\n",
      "DeepSeek-R1 has openly shared distilled checkpoints for various model sizes (1.5B, 7B, 8B, 14B, 32B, and 70B), based on the Qwen2.5 and Llama3 families. This allows the community to leverage these distillation techniques and build lighter, more accessible solutions, enabling deployment across different devices.\n",
      "\n",
      "![DeepSeek R1 Training Flow](https://i.gyazo.com/ad570b8608fea997c1c2bf5dc6f1e51f.png)\n",
      "\n",
      "![DeepSeek R1 Training Flow](https://i.gyazo.com/ad570b8608fea997c1c2bf5dc6f1e51f.png)\n",
      "\n",
      "### Aha Moment\n",
      "\n",
      "One of the most interesting discoveries during DeepSeek-R1-Zeros training phase is the emergence of a phenomenon known as the aha moment. In this intermediate stage:\n",
      "\n",
      "For researchers, this moment highlights the beauty and power of RL, as the model is not explicitly taught a step-by-step strategyit discovers it by receiving well-designed rewards.\n",
      "\n",
      "![Aha Moment from the Paper](https://i.gyazo.com/356f73a15cab8fac33d88560bff7ed59.png)\n",
      "\n",
      "![Aha Moment from the Paper](https://i.gyazo.com/356f73a15cab8fac33d88560bff7ed59.png)\n",
      "\n",
      "## Performance of DeepSeek R1\n",
      "\n",
      "The performance results of DeepSeek-R1 and its variants are remarkable:\n",
      "\n",
      "These figures represent a significant advancement over previous open-source models, demonstrating the effectiveness of combining RL and explicit reasoning.\n",
      "\n",
      "![DeepSeek R1 Zero vs. OpenAI o1 Performance](https://i.gyazo.com/f8eb1c168205738bc59b0389ac650cd6.png)\n",
      "\n",
      "![DeepSeek R1 Zero vs. OpenAI o1 Performance](https://i.gyazo.com/f8eb1c168205738bc59b0389ac650cd6.png)\n",
      "\n",
      "![Performance of DeepSeek R1, R1-Zero, and OpenAI o1](https://i.gyazo.com/510a206ff6a525a95a6b4a01b0e22605.png)\n",
      "\n",
      "![Performance of DeepSeek R1, R1-Zero, and OpenAI o1](https://i.gyazo.com/510a206ff6a525a95a6b4a01b0e22605.png)\n",
      "\n",
      "## Consequences of the DeepSeek R1 Release\n",
      "\n",
      "The release of DeepSeek-R1 and DeepSeek-R1-Zero has had a significant impact on the LLM research and development community. Some of the most notable consequences include:\n",
      "\n",
      "This has led to:\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The race to achieve AGI (Artificial General Intelligence) has begun, and no one wants to be left behind. We are witnessing a period of rapid innovation and competition with the potential to reshape humanity and our daily lives.\n",
      "\n",
      "Sometimes, resource limitations can be an opportunity to discover new techniques and approaches. DeepSeek R1 is a clear example of how, with proper incentives and an innovative mindset, it is possible to develop reasoning models that rival months of research and millions of dollars in investment.\n",
      "\n",
      "What would happen if these scientists and developers had the same resources as OpenAI or Meta? What models could they develop? What problems could they solve? What impact would they have on society?\n",
      "\n",
      "Many have claimed that the AI bubble has burst and that we are in a crisis. But is this truly a crisis, or is it an opportunity for smaller companies to realize they, too, can participate in the race toward AGI?\n",
      "\n",
      "Whether AGI is a benefit or a danger to humanity remains to be seen. For now, let's enjoy the advancements and competition that are leading us toward a smarter and more promising future.\n",
      "\n",
      "### ndice\n",
      "\n",
      "## Otros posts que podran interesarte\n",
      "\n",
      "![Banner photo](https://res.cloudinary.com/pedromebo/image/upload/q_auto,f_auto,c_fill,ar_5:2,w_1200/pedromebo/banner/ruff_python_linter_banner \"Banner photo\")\n",
      "\n",
      "#### Ruff - El Linter y Formateador de Python ms rpido: gua y configuracin\n",
      "\n",
      "December 04, 2025\n",
      "\n",
      "Descubre Ruff, la herramienta todo en uno que reemplaza a Flake8, Black e isort. Aprende a usarlo, configurarlo y por qu es el nuevo estndar en Python.\n",
      "\n",
      "![Banner photo](https://res.cloudinary.com/pedromebo/image/upload/q_auto,f_auto,c_fill,ar_5:2,w_1200/pedromebo/banner/llm_semantic_cache \"Banner photo\")\n",
      "\n",
      "#### Cmo ahorrar costes y mejorar la latencia en LLMs: Cach semntica con LangChain y Redis\n",
      "\n",
      "October 30, 2025\n",
      "\n",
      "Descubre cmo ahorrar costes usando LLM con cach semntica implementandola con LangChain y Redis, comparativa, pros y contras y errores frecuentes\n",
      "\n",
      "![Banner photo](https://res.cloudinary.com/pedromebo/image/upload/q_auto,f_auto,c_fill,ar_5:2,w_1200/pedromebo/banner/uv_package_manager_python_banner \"Banner photo\")\n",
      "\n",
      "#### UV - Gestor de paquetes de Python: cmo usarlo, comparativa y errores frecuentes\n",
      "\n",
      "October 21, 2025\n",
      "\n",
      "Descubre cmo usar UV el gestor de paquetes de Python que reemplazar a Pip, cuales son sus ventajas, comparativa con otros sistemas y errores frecuentes\n",
      "\n",
      "### Disfrutando del post?\n",
      "\n",
      "No dudes en contactarme si tienes alguna duda, sugerencia o proyecto que pueda ayudarte a hacerlo realidad.\n",
      "\n",
      "Contctame\n",
      "\n",
      " Pedromebo 2025  \n",
      "[My Source Code](https://github.com/pedromebo/pedromebo.com)</raw></document>\n",
      "<document><title>Who is rewarding DeepSeek R1? In RL you need some ...</title><url>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</url><content>Two things they used for rewards are rule-based math solvers and LeetCode problems with test cases.</content></document>\n",
      "<document><title>Bite: How Deepseek R1 was trained</title><url>https://www.philschmid.de/deepseek-r1</url><content>Group-Based Advantage: GRPO uses the average reward of a group of outputs as a baseline. This approach better aligns with the nature of reward</content><raw>![logo](/_next/image?url=%2Fstatic%2Flogo.png&w=48&q=75)\n",
      "\n",
      "# Bite: How Deepseek R1 was trained\n",
      "\n",
      "DeepSeek AI released DeepSeek-R1, an open model that rivals OpenAI's o1 in complex reasoning tasks, introduced using Group Relative Policy Optimization (GRPO) and RL-focused multi-stage training approach.\n",
      "\n",
      "## Understanding Group Relative Policy Optimization (GRPO)\n",
      "\n",
      "Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs. It was introduced in the [DeepSeekMath](https://arxiv.org/abs/2402.03300) paper in the context of mathematical reasoning. GRPO modifies the traditional Proximal Policy Optimization (PPO) by eliminating the need for a value function model. Instead, it estimates baselines from group scores, reducing memory usage and computational overhead. GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards as well as General Reward Models to improve models on helpfulness.\n",
      "\n",
      "![grpo.png](/static/blog/deepseek-r1/grpo.png)\n",
      "\n",
      "![grpo.png](/static/blog/deepseek-r1/grpo.png)\n",
      "\n",
      "The Key Differences from Proximal Policy Optimization (PPO) are\n",
      "\n",
      "## Exhibit: Pure Reinforcement Learning (R1-zero)\n",
      "\n",
      "In building DeepSeek R1, the team gained deep insights from experimenting with reinforcement learning on their base model. Starting with DeepSeek V3, they applied GRPO to unsupervised reasoning text completions rule-based reward models that focused on aspects like format, mathematics, and coding:\n",
      "\n",
      "![prompt.png](/static/blog/deepseek-r1/prompt.png)\n",
      "\n",
      "![prompt.png](/static/blog/deepseek-r1/prompt.png)\n",
      "\n",
      "This leads to a pass@1 score on AIME 2024 increasing from 15.6% to 71.0%, reaching performance levels comparable to OpenAI-o1-0912 alongside output token length per problem increasing, indicating the model naturally learns to solve tasks with more thinking time/token generation.\n",
      "\n",
      "![r1-zero.png](/static/blog/deepseek-r1/r1-zero.png)\n",
      "\n",
      "![r1-zero.png](/static/blog/deepseek-r1/r1-zero.png)\n",
      "\n",
      "This has the drawback of leading to poor readability and language mixing but it was solved for R1 using a multi-stage approach with alternating SFT  RL steps.\n",
      "\n",
      "## The Multi-Stage Training of DeepSeek R1\n",
      "\n",
      "To prevent the early unstable cold start phase of reinforcement training (RL) training from the base model, the team started with supervised fine-tuning.\n",
      "\n",
      "**Stage 1/4 Base to Supervised Fine-Tuning (SFT)**\n",
      "\n",
      "Collected up to 10k token-long chain-of-thought (CoT) using the fine-tuned models, R1-zero and human annotator. The data is used to fine-tune Deepseek V3 base to improve readbility and coherence.\n",
      "\n",
      "**Stage 2/4 RL for Reasoning**\n",
      "\n",
      "Used the same RL pipeline as R1-Zero, focusing on reasoning-intensive tasks such as coding and math using the same Rule-Based Reward Models. This time, an additional reward for \"language consistency\" is used to help the model stick to the same language.\n",
      "\n",
      "**Stage 3/4 Rejection Sampling and SFT**\n",
      "\n",
      "Generated large synthetic dataset using Reject Sampling (RS) focusing on writing, role-playing, and other general-purpose tasks. The model from Stage 2 was used with Deepseek V3 as a Judge to generate 600k reasoning-related samples and 200k for writing, role-playing, and other general-purpose tasks using portions of the SFT dataset of DeepSeek-V3 or regenerating them with CoT included.\n",
      "\n",
      "**Stage 4/4 RL for Helpfulness**\n",
      "\n",
      "In the Final Stage, GRPO is used again with a combination of Rule-Based and Outcome Reward Models to improve the model's helpfulness and harmlessness. Leading to the [Deepseek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) model.\n",
      "\n",
      "![evals.png](/static/blog/deepseek-r1/evals.png)\n",
      "\n",
      "![evals.png](/static/blog/deepseek-r1/evals.png)\n",
      "\n",
      "## Surprises</raw></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "The two types of rewards used in training DeepSeek-R1-Zero are:\n",
      "\n",
      "1. **Accuracy Rewards**: These evaluate whether the model's response is correct. For example, in math problems, the final answer must be in a specified format to allow rule-based verification. For coding problems like LeetCode, a compiler runs predefined test cases to provide feedback.\n",
      "\n",
      "2. **Format Rewards**: These enforce the model to output its reasoning process enclosed between `<think>` and `</think>` tags, ensuring the reasoning steps are clearly structured.\n",
      "\n",
      "These rewards guide the reinforcement learning process by incentivizing correctness and structured reasoning without using neural reward models, which were avoided to prevent reward hacking and reduce training complexity.\n",
      "\n",
      "**Source:**\n",
      "- data/Deepseek-r1.pdf (pages 5-6)\n",
      "('user', 'What are the two types of rewards used in training DeepSeek-R1-Zero, and what purposes do they serve?')\n",
      "('assistant', \"The two types of rewards used in training DeepSeek-R1-Zero are:\\n\\n1. **Accuracy Rewards**: These evaluate whether the model's response is correct. For example, in math problems, the final answer must be in a specified format to allow rule-based verification. For coding problems like LeetCode, a compiler runs predefined test cases to provide feedback.\\n\\n2. **Format Rewards**: These enforce the model to output its reasoning process enclosed between `<think>` and `</think>` tags, ensuring the reasoning steps are clearly structured.\\n\\nThese rewards guide the reinforcement learning process by incentivizing correctness and structured reasoning without using neural reward models, which were avoided to prevent reward hacking and reduce training complexity.\\n\\n**Source:**\\n- data/Deepseek-r1.pdf (pages 5-6)\")\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = GraphState(question=[HumanMessage(content=\"What two types of rewards are used to train DeepSeek-R1-Zero, and what is their purpose?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9ce127c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the Pass@1 performance of DeepSeek-R1 on the AIME 2024 and MATH-500 datasets?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths</content><source>data/Deepseek-r1.pdf</source><page>13</page></document>\n",
      "<document><content>On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n",
      "\n",
      "# 3.2. Distilled Model Evaluation</content><source>data/Deepseek-r1.pdf</source><page>14</page></document>\n",
      "<document><content>DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities</content><source>data/Deepseek-r1.pdf</source><page>7</page></document>\n",
      "<document><content>- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum of 32,768</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "<document><content>We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n",
      "\n",
      "In the future, we plan to invest in research across the following directions for DeepSeek-R1.</content><source>data/Deepseek-r1.pdf</source><page>16</page></document>\n",
      "<document><content>- Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content># 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n",
      "\n",
      "Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time.\n",
      "\n",
      "|   |\n",
      "| - |</content><source>data/Deepseek-r1.pdf</source><page>6</page></document>\n",
      "<document><content># Evaluation Setup\n",
      "\n",
      "We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@ evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top- value of 0.95 to generate  responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as</content><source>data/Deepseek-r1.pdf</source><page>12</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "The Pass@1 performance of DeepSeek-R1 is:\n",
      "\n",
      "- **AIME 2024:** 72.6%\n",
      "- **MATH-500:** 94.3%\n",
      "\n",
      "**Source:**\n",
      "- data/Deepseek-r1.pdf (page 4)\n",
      "('user', 'What is the Pass@1 performance of DeepSeek-R1 on the AIME 2024 and MATH-500 datasets?')\n",
      "('assistant', 'The Pass@1 performance of DeepSeek-R1 is:\\n\\n- **AIME 2024:** 72.6%\\n- **MATH-500:** 94.3%\\n\\n**Source:**\\n- data/Deepseek-r1.pdf (page 4)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = GraphState(question=[HumanMessage(content=\"What Pass@1 performance does DeepSeek-R1 achieve on AIME 2024 and MATH-500?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cf8c4e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mquery_rewrite\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "In what ways does distillation from DeepSeek-R1 enhance the performance of smaller models like Qwen-7B and Qwen-32B?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mretrieve\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mcontext\u001b[0m:\n",
      "<document><content>However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "<document><content>As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.\n",
      "\n",
      "# 4. Discussion</content><source>data/Deepseek-r1.pdf</source><page>14</page></document>\n",
      "<document><content>- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content># 4. Discussion\n",
      "\n",
      "# 4.1. Distillation v.s. Reinforcement Learning\n",
      "\n",
      "In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n",
      "\n",
      "To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale</content><source>data/Deepseek-r1.pdf</source><page>14</page></document>\n",
      "<document><content>RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content># 4.2. Unsuccessful Attempts\n",
      "\n",
      "In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n",
      "\n",
      "# Process Reward Model (PRM)</content><source>data/Deepseek-r1.pdf</source><page>15</page></document>\n",
      "<document><content>Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning</content><source>data/Deepseek-r1.pdf</source><page>5</page></document>\n",
      "<document><content>On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited.\n",
      "\n",
      "# 3.2. Distilled Model Evaluation</content><source>data/Deepseek-r1.pdf</source><page>14</page></document>\n",
      "<document><content>- We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.</content><source>data/Deepseek-r1.pdf</source><page>4</page></document>\n",
      "<document><content>We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.</content><source>data/Deepseek-r1.pdf</source><page>3</page></document>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mrelevance_check\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "yes\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Node: \u001b[1;36mllm_answer\u001b[0m \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "Distillation from DeepSeek-R1 enhances smaller models like Qwen-7B and Qwen-32B by transferring the advanced reasoning patterns learned by the larger DeepSeek-R1 model. This results in significantly improved performance on reasoning benchmarks compared to models trained solely with large-scale RL or non-reasoning baselines. For example, DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview, while DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024 and 94.3% on MATH-500, outperforming previous open-source models and matching or exceeding larger models. Distillation is more computationally efficient and yields better results than RL training on smaller models, highlighting the importance of leveraging reasoning capabilities from powerful base models.\n",
      "\n",
      "**Source:**\n",
      "- data/Deepseek-r1.pdf (pages 3, 4, 14, 15)\n",
      "('user', 'In what ways does distillation from DeepSeek-R1 enhance the performance of smaller models like Qwen-7B and Qwen-32B?')\n",
      "('assistant', 'Distillation from DeepSeek-R1 enhances smaller models like Qwen-7B and Qwen-32B by transferring the advanced reasoning patterns learned by the larger DeepSeek-R1 model. This results in significantly improved performance on reasoning benchmarks compared to models trained solely with large-scale RL or non-reasoning baselines. For example, DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview, while DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024 and 94.3% on MATH-500, outperforming previous open-source models and matching or exceeding larger models. Distillation is more computationally efficient and yields better results than RL training on smaller models, highlighting the importance of leveraging reasoning capabilities from powerful base models.\\n\\n**Source:**\\n- data/Deepseek-r1.pdf (pages 3, 4, 14, 15)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "inputs = GraphState(question=[HumanMessage(content=\"How does distillation from DeepSeek-R1 improve the performance of smaller models such as Qwen-7B or Qwen-32B?\")])\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc84df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
